{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -qq transformers torch accelerate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5sbE-BKVTgk",
        "outputId": "a758007d-9c6a-406e-e525-04fc433fc1dc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.31.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Collecting accelerate\n",
            "  Downloading accelerate-0.21.0-py3-none-any.whl (244 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.16.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.6)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: accelerate\n",
            "Successfully installed accelerate-0.21.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build starcoder.cpp project"
      ],
      "metadata": {
        "id": "T-Vd1boYWmwp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p1A90KJyUxt1",
        "outputId": "e520c805-b141-4e33-96c2-6a8540e4b10b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'starcoder.cpp'...\n",
            "remote: Enumerating objects: 103, done.\u001b[K\n",
            "remote: Counting objects: 100% (103/103), done.\u001b[K\n",
            "remote: Compressing objects: 100% (73/73), done.\u001b[K\n",
            "remote: Total 103 (delta 30), reused 91 (delta 24), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (103/103), 7.28 MiB | 18.49 MiB/s, done.\n",
            "Resolving deltas: 100% (30/30), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/bigcode-project/starcoder.cpp"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd starcoder.cpp && make"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S_FL3SpHWgSr",
        "outputId": "f3c216d7-3e8a-4fcf-ad25-40bac9f4efff"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I llama.cpp build info: \n",
            "I UNAME_S:  Linux\n",
            "I UNAME_P:  x86_64\n",
            "I UNAME_M:  x86_64\n",
            "I CFLAGS:   -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native\n",
            "I CXXFLAGS: -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native\n",
            "I LDFLAGS:  \n",
            "I CC:       cc (Ubuntu 11.3.0-1ubuntu1~22.04.1) 11.3.0\n",
            "I CXX:      g++ (Ubuntu 11.3.0-1ubuntu1~22.04.1) 11.3.0\n",
            "\n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -c common.cpp -o common.o\n",
            "cc  -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native   -c ggml.c -o ggml.o\n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native main.cpp common.o ggml.o -o main \n",
            "\u001b[01m\u001b[Kmain.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kbool starcoder_model_load(const string&, starcoder_model&, gpt_vocab&)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[Kmain.cpp:207:13:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[KC++ designated initializers only available with ‘\u001b[01m\u001b[K-std=c++20\u001b[m\u001b[K’ or ‘\u001b[01m\u001b[K-std=gnu++20\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wpedantic\u0007-Wpedantic\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  207 |             \u001b[01;35m\u001b[K.\u001b[m\u001b[Kmem_size   = ctx_size,\n",
            "      |             \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kmain.cpp:208:13:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[KC++ designated initializers only available with ‘\u001b[01m\u001b[K-std=c++20\u001b[m\u001b[K’ or ‘\u001b[01m\u001b[K-std=gnu++20\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wpedantic\u0007-Wpedantic\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  208 |             \u001b[01;35m\u001b[K.\u001b[m\u001b[Kmem_buffer = NULL,\n",
            "      |             \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kmain.cpp:209:13:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[KC++ designated initializers only available with ‘\u001b[01m\u001b[K-std=c++20\u001b[m\u001b[K’ or ‘\u001b[01m\u001b[K-std=gnu++20\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wpedantic\u0007-Wpedantic\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  209 |             \u001b[01;35m\u001b[K.\u001b[m\u001b[Kno_alloc   = false,\n",
            "      |             \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kmain.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kbool starcoder_eval(const starcoder_model&, int, int, const std::vector<int>&, std::vector<float>&, size_t&)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[Kmain.cpp:441:9:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[KC++ designated initializers only available with ‘\u001b[01m\u001b[K-std=c++20\u001b[m\u001b[K’ or ‘\u001b[01m\u001b[K-std=gnu++20\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wpedantic\u0007-Wpedantic\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  441 |         \u001b[01;35m\u001b[K.\u001b[m\u001b[Kmem_size   = buf_size,\n",
            "      |         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kmain.cpp:442:9:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[KC++ designated initializers only available with ‘\u001b[01m\u001b[K-std=c++20\u001b[m\u001b[K’ or ‘\u001b[01m\u001b[K-std=gnu++20\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wpedantic\u0007-Wpedantic\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  442 |         \u001b[01;35m\u001b[K.\u001b[m\u001b[Kmem_buffer = buf,\n",
            "      |         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kmain.cpp:443:9:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[KC++ designated initializers only available with ‘\u001b[01m\u001b[K-std=c++20\u001b[m\u001b[K’ or ‘\u001b[01m\u001b[K-std=gnu++20\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wpedantic\u0007-Wpedantic\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  443 |         \u001b[01;35m\u001b[K.\u001b[m\u001b[Kno_alloc   = false,\n",
            "      |         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kmain.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kint main(int, char**)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[Kmain.cpp:799:33:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison of integer expressions of different signedness: ‘\u001b[01m\u001b[Kint\u001b[m\u001b[K’ and ‘\u001b[01m\u001b[Kstd::vector<int>::size_type\u001b[m\u001b[K’ {aka ‘\u001b[01m\u001b[Klong unsigned int\u001b[m\u001b[K’} [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wsign-compare\u0007-Wsign-compare\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  799 |     for (int i = embd.size(); \u001b[01;35m\u001b[Ki < embd_inp.size() + params.n_predict\u001b[m\u001b[K; i++) {\n",
            "      |                               \u001b[01;35m\u001b[K~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kmain.cpp:815:15:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison of integer expressions of different signedness: ‘\u001b[01m\u001b[Kint\u001b[m\u001b[K’ and ‘\u001b[01m\u001b[Kstd::vector<int>::size_type\u001b[m\u001b[K’ {aka ‘\u001b[01m\u001b[Klong unsigned int\u001b[m\u001b[K’} [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wsign-compare\u0007-Wsign-compare\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  815 |         if (\u001b[01;35m\u001b[Ki >= embd_inp.size()\u001b[m\u001b[K) {\n",
            "      |             \u001b[01;35m\u001b[K~~^~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kmain.cpp:837:31:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison of integer expressions of different signedness: ‘\u001b[01m\u001b[Kint\u001b[m\u001b[K’ and ‘\u001b[01m\u001b[Kstd::vector<int>::size_type\u001b[m\u001b[K’ {aka ‘\u001b[01m\u001b[Klong unsigned int\u001b[m\u001b[K’} [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wsign-compare\u0007-Wsign-compare\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  837 |             for (int k = i; \u001b[01;35m\u001b[Kk < embd_inp.size()\u001b[m\u001b[K; k++) {\n",
            "      |                             \u001b[01;35m\u001b[K~~^~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kmain.cpp:839:33:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison of integer expressions of different signedness: ‘\u001b[01m\u001b[Kstd::vector<int>::size_type\u001b[m\u001b[K’ {aka ‘\u001b[01m\u001b[Klong unsigned int\u001b[m\u001b[K’} and ‘\u001b[01m\u001b[Kint32_t\u001b[m\u001b[K’ {aka ‘\u001b[01m\u001b[Kint\u001b[m\u001b[K’} [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wsign-compare\u0007-Wsign-compare\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  839 |                 if (\u001b[01;35m\u001b[Kembd.size() >= params.n_batch\u001b[m\u001b[K) {\n",
            "      |                     \u001b[01;35m\u001b[K~~~~~~~~~~~~^~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\n",
            "====  Run ./main -h for help.  ====\n",
            "\n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native quantize.cpp ggml.o common.o -o quantize \n",
            "\u001b[01m\u001b[Kquantize.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kbool starcoder_model_quantize(const string&, const string&, ggml_ftype)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[Kquantize.cpp:85:20:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcast from type ‘\u001b[01m\u001b[Kconst int32_t*\u001b[m\u001b[K’ {aka ‘\u001b[01m\u001b[Kconst int*\u001b[m\u001b[K’} to type ‘\u001b[01m\u001b[Kchar*\u001b[m\u001b[K’ casts away qualifiers [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wcast-qual\u0007-Wcast-qual\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   85 |         fout.write(\u001b[01;35m\u001b[K(char *) &ftype_dst\u001b[m\u001b[K,       sizeof(ftype_dst));\n",
            "      |                    \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kquantize.cpp:107:24:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcast from type ‘\u001b[01m\u001b[Kconst char*\u001b[m\u001b[K’ to type ‘\u001b[01m\u001b[Kchar*\u001b[m\u001b[K’ casts away qualifiers [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wcast-qual\u0007-Wcast-qual\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  107 |             finp.read (\u001b[01;35m\u001b[K(char *) word.data()\u001b[m\u001b[K, len);\n",
            "      |                        \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kquantize.cpp:108:24:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcast from type ‘\u001b[01m\u001b[Kconst char*\u001b[m\u001b[K’ to type ‘\u001b[01m\u001b[Kchar*\u001b[m\u001b[K’ casts away qualifiers [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wcast-qual\u0007-Wcast-qual\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  108 |             fout.write(\u001b[01;35m\u001b[K(char *) word.data()\u001b[m\u001b[K, len);\n",
            "      |                        \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download Model and convert it to GGML format"
      ],
      "metadata": {
        "id": "3d69BVJdWrGT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python starcoder.cpp/convert-hf-to-ggml.py rahuldshetty/tiny-starcoder-instruct"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RAFJEmcOVDhM",
        "outputId": "156320dc-87c5-4ba3-8881-52a42f93bca4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-07-22 11:26:25.629875: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Loading model:  rahuldshetty/tiny-starcoder-instruct\n",
            "Downloading pytorch_model.bin: 100% 657M/657M [00:09<00:00, 67.4MB/s]\n",
            "Downloading (…)neration_config.json: 100% 111/111 [00:00<00:00, 622kB/s]\n",
            "Model loaded:  rahuldshetty/tiny-starcoder-instruct\n",
            "{'vocab_size': 49152, 'n_positions': 8192, 'n_embd': 768, 'n_layer': 20, 'n_head': 12, 'n_inner': 3072, 'activation_function': 'gelu_pytorch_tanh', 'resid_pdrop': 0.1, 'embd_pdrop': 0.1, 'attn_pdrop': 0.1, 'layer_norm_epsilon': 1e-05, 'initializer_range': 0.02, 'scale_attn_weights': True, 'use_cache': False, 'attention_softmax_in_fp32': True, 'scale_attention_softmax_in_fp32': True, 'multi_query': True, 'bos_token_id': 0, 'eos_token_id': 0, 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'chunk_size_feed_forward': 0, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['GPTBigCodeForCausalLM'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'pad_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'rahuldshetty/tiny-starcoder-instruct', 'transformers_version': '4.31.0', 'inference_runner': 0, 'max_batch_size': None, 'max_sequence_length': None, 'model_type': 'gpt_bigcode', 'pad_key_length': True, 'pre_allocate_kv_cache': False, 'summary_activation': None, 'summary_first_dropout': 0.1, 'summary_proj_to_labels': True, 'summary_type': 'cls_index', 'summary_use_proj': True, 'validate_runner_input': True}\n",
            "Saving ggml model to:  models/rahuldshetty/tiny-starcoder-instruct-ggml.bin\n",
            "Processing variable: transformer.wte.weight with shape:  (49152, 768)\n",
            "  Converting to float16\n",
            "Processing variable: transformer.wpe.weight with shape:  (8192, 768)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.0.ln_1.weight with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.0.ln_1.bias with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.0.attn.c_attn.weight with shape:  (896, 768)\n",
            "  Converting to float16\n",
            "  Duplicate K,V heads to use MHA instead of MQA\n",
            "Processing variable: transformer.h.0.attn.c_attn.bias with shape:  (896,)\n",
            "  Converting to float32\n",
            "  Duplicate K,V heads to use MHA instead of MQA\n",
            "Processing variable: transformer.h.0.attn.c_proj.weight with shape:  (768, 768)\n",
            "  Converting to float16\n",
            "Processing variable: transformer.h.0.attn.c_proj.bias with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.0.ln_2.weight with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.0.ln_2.bias with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.0.mlp.c_fc.weight with shape:  (3072, 768)\n",
            "  Converting to float16\n",
            "Processing variable: transformer.h.0.mlp.c_fc.bias with shape:  (3072,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.0.mlp.c_proj.weight with shape:  (768, 3072)\n",
            "  Converting to float16\n",
            "Processing variable: transformer.h.0.mlp.c_proj.bias with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.1.ln_1.weight with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.1.ln_1.bias with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.1.attn.c_attn.weight with shape:  (896, 768)\n",
            "  Converting to float16\n",
            "  Duplicate K,V heads to use MHA instead of MQA\n",
            "Processing variable: transformer.h.1.attn.c_attn.bias with shape:  (896,)\n",
            "  Converting to float32\n",
            "  Duplicate K,V heads to use MHA instead of MQA\n",
            "Processing variable: transformer.h.1.attn.c_proj.weight with shape:  (768, 768)\n",
            "  Converting to float16\n",
            "Processing variable: transformer.h.1.attn.c_proj.bias with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.1.ln_2.weight with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.1.ln_2.bias with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.1.mlp.c_fc.weight with shape:  (3072, 768)\n",
            "  Converting to float16\n",
            "Processing variable: transformer.h.1.mlp.c_fc.bias with shape:  (3072,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.1.mlp.c_proj.weight with shape:  (768, 3072)\n",
            "  Converting to float16\n",
            "Processing variable: transformer.h.1.mlp.c_proj.bias with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.2.ln_1.weight with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.2.ln_1.bias with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.2.attn.c_attn.weight with shape:  (896, 768)\n",
            "  Converting to float16\n",
            "  Duplicate K,V heads to use MHA instead of MQA\n",
            "Processing variable: transformer.h.2.attn.c_attn.bias with shape:  (896,)\n",
            "  Converting to float32\n",
            "  Duplicate K,V heads to use MHA instead of MQA\n",
            "Processing variable: transformer.h.2.attn.c_proj.weight with shape:  (768, 768)\n",
            "  Converting to float16\n",
            "Processing variable: transformer.h.2.attn.c_proj.bias with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.2.ln_2.weight with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.2.ln_2.bias with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.2.mlp.c_fc.weight with shape:  (3072, 768)\n",
            "  Converting to float16\n",
            "Processing variable: transformer.h.2.mlp.c_fc.bias with shape:  (3072,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.2.mlp.c_proj.weight with shape:  (768, 3072)\n",
            "  Converting to float16\n",
            "Processing variable: transformer.h.2.mlp.c_proj.bias with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.3.ln_1.weight with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.3.ln_1.bias with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.3.attn.c_attn.weight with shape:  (896, 768)\n",
            "  Converting to float16\n",
            "  Duplicate K,V heads to use MHA instead of MQA\n",
            "Processing variable: transformer.h.3.attn.c_attn.bias with shape:  (896,)\n",
            "  Converting to float32\n",
            "  Duplicate K,V heads to use MHA instead of MQA\n",
            "Processing variable: transformer.h.3.attn.c_proj.weight with shape:  (768, 768)\n",
            "  Converting to float16\n",
            "Processing variable: transformer.h.3.attn.c_proj.bias with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.3.ln_2.weight with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.3.ln_2.bias with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.3.mlp.c_fc.weight with shape:  (3072, 768)\n",
            "  Converting to float16\n",
            "Processing variable: transformer.h.3.mlp.c_fc.bias with shape:  (3072,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.3.mlp.c_proj.weight with shape:  (768, 3072)\n",
            "  Converting to float16\n",
            "Processing variable: transformer.h.3.mlp.c_proj.bias with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.4.ln_1.weight with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.4.ln_1.bias with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.4.attn.c_attn.weight with shape:  (896, 768)\n",
            "  Converting to float16\n",
            "  Duplicate K,V heads to use MHA instead of MQA\n",
            "Processing variable: transformer.h.4.attn.c_attn.bias with shape:  (896,)\n",
            "  Converting to float32\n",
            "  Duplicate K,V heads to use MHA instead of MQA\n",
            "Processing variable: transformer.h.4.attn.c_proj.weight with shape:  (768, 768)\n",
            "  Converting to float16\n",
            "Processing variable: transformer.h.4.attn.c_proj.bias with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.4.ln_2.weight with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.4.ln_2.bias with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.4.mlp.c_fc.weight with shape:  (3072, 768)\n",
            "  Converting to float16\n",
            "Processing variable: transformer.h.4.mlp.c_fc.bias with shape:  (3072,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.4.mlp.c_proj.weight with shape:  (768, 3072)\n",
            "  Converting to float16\n",
            "Processing variable: transformer.h.4.mlp.c_proj.bias with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.5.ln_1.weight with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.5.ln_1.bias with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.5.attn.c_attn.weight with shape:  (896, 768)\n",
            "  Converting to float16\n",
            "  Duplicate K,V heads to use MHA instead of MQA\n",
            "Processing variable: transformer.h.5.attn.c_attn.bias with shape:  (896,)\n",
            "  Converting to float32\n",
            "  Duplicate K,V heads to use MHA instead of MQA\n",
            "Processing variable: transformer.h.5.attn.c_proj.weight with shape:  (768, 768)\n",
            "  Converting to float16\n",
            "Processing variable: transformer.h.5.attn.c_proj.bias with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.5.ln_2.weight with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.5.ln_2.bias with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.5.mlp.c_fc.weight with shape:  (3072, 768)\n",
            "  Converting to float16\n",
            "Processing variable: transformer.h.5.mlp.c_fc.bias with shape:  (3072,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.5.mlp.c_proj.weight with shape:  (768, 3072)\n",
            "  Converting to float16\n",
            "Processing variable: transformer.h.5.mlp.c_proj.bias with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.6.ln_1.weight with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.6.ln_1.bias with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.6.attn.c_attn.weight with shape:  (896, 768)\n",
            "  Converting to float16\n",
            "  Duplicate K,V heads to use MHA instead of MQA\n",
            "Processing variable: transformer.h.6.attn.c_attn.bias with shape:  (896,)\n",
            "  Converting to float32\n",
            "  Duplicate K,V heads to use MHA instead of MQA\n",
            "Processing variable: transformer.h.6.attn.c_proj.weight with shape:  (768, 768)\n",
            "  Converting to float16\n",
            "Processing variable: transformer.h.6.attn.c_proj.bias with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.6.ln_2.weight with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.6.ln_2.bias with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.6.mlp.c_fc.weight with shape:  (3072, 768)\n",
            "  Converting to float16\n",
            "Processing variable: transformer.h.6.mlp.c_fc.bias with shape:  (3072,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.6.mlp.c_proj.weight with shape:  (768, 3072)\n",
            "  Converting to float16\n",
            "Processing variable: transformer.h.6.mlp.c_proj.bias with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.7.ln_1.weight with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.7.ln_1.bias with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.7.attn.c_attn.weight with shape:  (896, 768)\n",
            "  Converting to float16\n",
            "  Duplicate K,V heads to use MHA instead of MQA\n",
            "Processing variable: transformer.h.7.attn.c_attn.bias with shape:  (896,)\n",
            "  Converting to float32\n",
            "  Duplicate K,V heads to use MHA instead of MQA\n",
            "Processing variable: transformer.h.7.attn.c_proj.weight with shape:  (768, 768)\n",
            "  Converting to float16\n",
            "Processing variable: transformer.h.7.attn.c_proj.bias with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.7.ln_2.weight with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.7.ln_2.bias with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.7.mlp.c_fc.weight with shape:  (3072, 768)\n",
            "  Converting to float16\n",
            "Processing variable: transformer.h.7.mlp.c_fc.bias with shape:  (3072,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.7.mlp.c_proj.weight with shape:  (768, 3072)\n",
            "  Converting to float16\n",
            "Processing variable: transformer.h.7.mlp.c_proj.bias with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.8.ln_1.weight with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.8.ln_1.bias with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.8.attn.c_attn.weight with shape:  (896, 768)\n",
            "  Converting to float16\n",
            "  Duplicate K,V heads to use MHA instead of MQA\n",
            "Processing variable: transformer.h.8.attn.c_attn.bias with shape:  (896,)\n",
            "  Converting to float32\n",
            "  Duplicate K,V heads to use MHA instead of MQA\n",
            "Processing variable: transformer.h.8.attn.c_proj.weight with shape:  (768, 768)\n",
            "  Converting to float16\n",
            "Processing variable: transformer.h.8.attn.c_proj.bias with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.8.ln_2.weight with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.8.ln_2.bias with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.8.mlp.c_fc.weight with shape:  (3072, 768)\n",
            "  Converting to float16\n",
            "Processing variable: transformer.h.8.mlp.c_fc.bias with shape:  (3072,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.8.mlp.c_proj.weight with shape:  (768, 3072)\n",
            "  Converting to float16\n",
            "Processing variable: transformer.h.8.mlp.c_proj.bias with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.9.ln_1.weight with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.9.ln_1.bias with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.9.attn.c_attn.weight with shape:  (896, 768)\n",
            "  Converting to float16\n",
            "  Duplicate K,V heads to use MHA instead of MQA\n",
            "Processing variable: transformer.h.9.attn.c_attn.bias with shape:  (896,)\n",
            "  Converting to float32\n",
            "  Duplicate K,V heads to use MHA instead of MQA\n",
            "Processing variable: transformer.h.9.attn.c_proj.weight with shape:  (768, 768)\n",
            "  Converting to float16\n",
            "Processing variable: transformer.h.9.attn.c_proj.bias with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.9.ln_2.weight with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.9.ln_2.bias with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.9.mlp.c_fc.weight with shape:  (3072, 768)\n",
            "  Converting to float16\n",
            "Processing variable: transformer.h.9.mlp.c_fc.bias with shape:  (3072,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.9.mlp.c_proj.weight with shape:  (768, 3072)\n",
            "  Converting to float16\n",
            "Processing variable: transformer.h.9.mlp.c_proj.bias with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.10.ln_1.weight with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.10.ln_1.bias with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.10.attn.c_attn.weight with shape:  (896, 768)\n",
            "  Converting to float16\n",
            "  Duplicate K,V heads to use MHA instead of MQA\n",
            "Processing variable: transformer.h.10.attn.c_attn.bias with shape:  (896,)\n",
            "  Converting to float32\n",
            "  Duplicate K,V heads to use MHA instead of MQA\n",
            "Processing variable: transformer.h.10.attn.c_proj.weight with shape:  (768, 768)\n",
            "  Converting to float16\n",
            "Processing variable: transformer.h.10.attn.c_proj.bias with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.10.ln_2.weight with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.10.ln_2.bias with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.10.mlp.c_fc.weight with shape:  (3072, 768)\n",
            "  Converting to float16\n",
            "Processing variable: transformer.h.10.mlp.c_fc.bias with shape:  (3072,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.10.mlp.c_proj.weight with shape:  (768, 3072)\n",
            "  Converting to float16\n",
            "Processing variable: transformer.h.10.mlp.c_proj.bias with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.11.ln_1.weight with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.11.ln_1.bias with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.11.attn.c_attn.weight with shape:  (896, 768)\n",
            "  Converting to float16\n",
            "  Duplicate K,V heads to use MHA instead of MQA\n",
            "Processing variable: transformer.h.11.attn.c_attn.bias with shape:  (896,)\n",
            "  Converting to float32\n",
            "  Duplicate K,V heads to use MHA instead of MQA\n",
            "Processing variable: transformer.h.11.attn.c_proj.weight with shape:  (768, 768)\n",
            "  Converting to float16\n",
            "Processing variable: transformer.h.11.attn.c_proj.bias with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.11.ln_2.weight with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.11.ln_2.bias with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.11.mlp.c_fc.weight with shape:  (3072, 768)\n",
            "  Converting to float16\n",
            "Processing variable: transformer.h.11.mlp.c_fc.bias with shape:  (3072,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.11.mlp.c_proj.weight with shape:  (768, 3072)\n",
            "  Converting to float16\n",
            "Processing variable: transformer.h.11.mlp.c_proj.bias with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.12.ln_1.weight with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.12.ln_1.bias with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.12.attn.c_attn.weight with shape:  (896, 768)\n",
            "  Converting to float16\n",
            "  Duplicate K,V heads to use MHA instead of MQA\n",
            "Processing variable: transformer.h.12.attn.c_attn.bias with shape:  (896,)\n",
            "  Converting to float32\n",
            "  Duplicate K,V heads to use MHA instead of MQA\n",
            "Processing variable: transformer.h.12.attn.c_proj.weight with shape:  (768, 768)\n",
            "  Converting to float16\n",
            "Processing variable: transformer.h.12.attn.c_proj.bias with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.12.ln_2.weight with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.12.ln_2.bias with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.12.mlp.c_fc.weight with shape:  (3072, 768)\n",
            "  Converting to float16\n",
            "Processing variable: transformer.h.12.mlp.c_fc.bias with shape:  (3072,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.12.mlp.c_proj.weight with shape:  (768, 3072)\n",
            "  Converting to float16\n",
            "Processing variable: transformer.h.12.mlp.c_proj.bias with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.13.ln_1.weight with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.13.ln_1.bias with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.13.attn.c_attn.weight with shape:  (896, 768)\n",
            "  Converting to float16\n",
            "  Duplicate K,V heads to use MHA instead of MQA\n",
            "Processing variable: transformer.h.13.attn.c_attn.bias with shape:  (896,)\n",
            "  Converting to float32\n",
            "  Duplicate K,V heads to use MHA instead of MQA\n",
            "Processing variable: transformer.h.13.attn.c_proj.weight with shape:  (768, 768)\n",
            "  Converting to float16\n",
            "Processing variable: transformer.h.13.attn.c_proj.bias with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.13.ln_2.weight with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.13.ln_2.bias with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.13.mlp.c_fc.weight with shape:  (3072, 768)\n",
            "  Converting to float16\n",
            "Processing variable: transformer.h.13.mlp.c_fc.bias with shape:  (3072,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.13.mlp.c_proj.weight with shape:  (768, 3072)\n",
            "  Converting to float16\n",
            "Processing variable: transformer.h.13.mlp.c_proj.bias with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.14.ln_1.weight with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.14.ln_1.bias with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.14.attn.c_attn.weight with shape:  (896, 768)\n",
            "  Converting to float16\n",
            "  Duplicate K,V heads to use MHA instead of MQA\n",
            "Processing variable: transformer.h.14.attn.c_attn.bias with shape:  (896,)\n",
            "  Converting to float32\n",
            "  Duplicate K,V heads to use MHA instead of MQA\n",
            "Processing variable: transformer.h.14.attn.c_proj.weight with shape:  (768, 768)\n",
            "  Converting to float16\n",
            "Processing variable: transformer.h.14.attn.c_proj.bias with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.14.ln_2.weight with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.14.ln_2.bias with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.14.mlp.c_fc.weight with shape:  (3072, 768)\n",
            "  Converting to float16\n",
            "Processing variable: transformer.h.14.mlp.c_fc.bias with shape:  (3072,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.14.mlp.c_proj.weight with shape:  (768, 3072)\n",
            "  Converting to float16\n",
            "Processing variable: transformer.h.14.mlp.c_proj.bias with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.15.ln_1.weight with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.15.ln_1.bias with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.15.attn.c_attn.weight with shape:  (896, 768)\n",
            "  Converting to float16\n",
            "  Duplicate K,V heads to use MHA instead of MQA\n",
            "Processing variable: transformer.h.15.attn.c_attn.bias with shape:  (896,)\n",
            "  Converting to float32\n",
            "  Duplicate K,V heads to use MHA instead of MQA\n",
            "Processing variable: transformer.h.15.attn.c_proj.weight with shape:  (768, 768)\n",
            "  Converting to float16\n",
            "Processing variable: transformer.h.15.attn.c_proj.bias with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.15.ln_2.weight with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.15.ln_2.bias with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.15.mlp.c_fc.weight with shape:  (3072, 768)\n",
            "  Converting to float16\n",
            "Processing variable: transformer.h.15.mlp.c_fc.bias with shape:  (3072,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.15.mlp.c_proj.weight with shape:  (768, 3072)\n",
            "  Converting to float16\n",
            "Processing variable: transformer.h.15.mlp.c_proj.bias with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.16.ln_1.weight with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.16.ln_1.bias with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.16.attn.c_attn.weight with shape:  (896, 768)\n",
            "  Converting to float16\n",
            "  Duplicate K,V heads to use MHA instead of MQA\n",
            "Processing variable: transformer.h.16.attn.c_attn.bias with shape:  (896,)\n",
            "  Converting to float32\n",
            "  Duplicate K,V heads to use MHA instead of MQA\n",
            "Processing variable: transformer.h.16.attn.c_proj.weight with shape:  (768, 768)\n",
            "  Converting to float16\n",
            "Processing variable: transformer.h.16.attn.c_proj.bias with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.16.ln_2.weight with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.16.ln_2.bias with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.16.mlp.c_fc.weight with shape:  (3072, 768)\n",
            "  Converting to float16\n",
            "Processing variable: transformer.h.16.mlp.c_fc.bias with shape:  (3072,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.16.mlp.c_proj.weight with shape:  (768, 3072)\n",
            "  Converting to float16\n",
            "Processing variable: transformer.h.16.mlp.c_proj.bias with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.17.ln_1.weight with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.17.ln_1.bias with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.17.attn.c_attn.weight with shape:  (896, 768)\n",
            "  Converting to float16\n",
            "  Duplicate K,V heads to use MHA instead of MQA\n",
            "Processing variable: transformer.h.17.attn.c_attn.bias with shape:  (896,)\n",
            "  Converting to float32\n",
            "  Duplicate K,V heads to use MHA instead of MQA\n",
            "Processing variable: transformer.h.17.attn.c_proj.weight with shape:  (768, 768)\n",
            "  Converting to float16\n",
            "Processing variable: transformer.h.17.attn.c_proj.bias with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.17.ln_2.weight with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.17.ln_2.bias with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.17.mlp.c_fc.weight with shape:  (3072, 768)\n",
            "  Converting to float16\n",
            "Processing variable: transformer.h.17.mlp.c_fc.bias with shape:  (3072,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.17.mlp.c_proj.weight with shape:  (768, 3072)\n",
            "  Converting to float16\n",
            "Processing variable: transformer.h.17.mlp.c_proj.bias with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.18.ln_1.weight with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.18.ln_1.bias with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.18.attn.c_attn.weight with shape:  (896, 768)\n",
            "  Converting to float16\n",
            "  Duplicate K,V heads to use MHA instead of MQA\n",
            "Processing variable: transformer.h.18.attn.c_attn.bias with shape:  (896,)\n",
            "  Converting to float32\n",
            "  Duplicate K,V heads to use MHA instead of MQA\n",
            "Processing variable: transformer.h.18.attn.c_proj.weight with shape:  (768, 768)\n",
            "  Converting to float16\n",
            "Processing variable: transformer.h.18.attn.c_proj.bias with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.18.ln_2.weight with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.18.ln_2.bias with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.18.mlp.c_fc.weight with shape:  (3072, 768)\n",
            "  Converting to float16\n",
            "Processing variable: transformer.h.18.mlp.c_fc.bias with shape:  (3072,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.18.mlp.c_proj.weight with shape:  (768, 3072)\n",
            "  Converting to float16\n",
            "Processing variable: transformer.h.18.mlp.c_proj.bias with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.19.ln_1.weight with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.19.ln_1.bias with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.19.attn.c_attn.weight with shape:  (896, 768)\n",
            "  Converting to float16\n",
            "  Duplicate K,V heads to use MHA instead of MQA\n",
            "Processing variable: transformer.h.19.attn.c_attn.bias with shape:  (896,)\n",
            "  Converting to float32\n",
            "  Duplicate K,V heads to use MHA instead of MQA\n",
            "Processing variable: transformer.h.19.attn.c_proj.weight with shape:  (768, 768)\n",
            "  Converting to float16\n",
            "Processing variable: transformer.h.19.attn.c_proj.bias with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.19.ln_2.weight with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.19.ln_2.bias with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.19.mlp.c_fc.weight with shape:  (3072, 768)\n",
            "  Converting to float16\n",
            "Processing variable: transformer.h.19.mlp.c_fc.bias with shape:  (3072,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.h.19.mlp.c_proj.weight with shape:  (768, 3072)\n",
            "  Converting to float16\n",
            "Processing variable: transformer.h.19.mlp.c_proj.bias with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.ln_f.weight with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: transformer.ln_f.bias with shape:  (768,)\n",
            "  Converting to float32\n",
            "Processing variable: lm_head.weight with shape:  (49152, 768)\n",
            "  Converting to float16\n",
            "Done. Output file: models/rahuldshetty/tiny-starcoder-instruct-ggml.bin\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd starcoder.cpp && ./quantize --help"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yXuJ7LCUWWCC",
        "outputId": "a10d42e8-660d-4892-cce5-62fd418ac837"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: ./quantize model-f32.bin model-quant.bin type\n",
            "  type = \"q4_0\" or 2\n",
            "  type = \"q4_1\" or 3\n",
            "  type = \"q5_0\" or 8\n",
            "  type = \"q5_1\" or 9\n",
            "  type = \"q8_0\" or 7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd starcoder.cpp && ./quantize ../models/rahuldshetty/tiny-starcoder-instruct-ggml.bin ../quantized/tiny-starcoder-instruct-ggml-q4_0.bin 2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SykaeCEeWz7D",
        "outputId": "a03ddcc1-60f8-4abb-f5ff-104f71558601"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "starcoder_model_quantize: loading model from '../models/rahuldshetty/tiny-starcoder-instruct-ggml.bin'\n",
            "starcoder_model_quantize: n_vocab     = 49152\n",
            "starcoder_model_quantize: n_ctx       = 8192\n",
            "starcoder_model_quantize: n_embd      = 768\n",
            "starcoder_model_quantize: n_head      = 12\n",
            "starcoder_model_quantize: n_layer     = 20\n",
            "starcoder_model_quantize: ftype (src) = 1\n",
            "starcoder_model_quantize: qntvr (src) = 0\n",
            "starcoder_model_quantize: ftype (dst) = 1002\n",
            "starcoder_model_quantize: qntvr (dst) = 1\n",
            "                                                       model/wte - [  768, 49152,     1], type =    f16 size =   144.00 MB ->    22.50 MB | hist: 0.036 0.013 0.021 0.033 0.050 0.073 0.099 0.123 0.132 0.123 0.100 0.074 0.051 0.034 0.021 0.018 \n",
            "                                                       model/wpe - [  768,  8192,     1], type =    f32 size =   24.000 MB\n",
            "                                                 model/h0/ln_1/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h0/ln_1/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                          model/h0/attn/c_attn/w - [  768,  2304,     1], type =    f16 size =     6.75 MB ->     1.05 MB | hist: 0.036 0.015 0.024 0.038 0.057 0.075 0.098 0.114 0.120 0.113 0.098 0.074 0.056 0.038 0.024 0.020 \n",
            "                                          model/h0/attn/c_attn/b - [ 2304,     1,     1], type =    f32 size =    0.009 MB\n",
            "                                          model/h0/attn/c_proj/w - [  768,   768,     1], type =    f16 size =     2.25 MB ->     0.35 MB | hist: 0.037 0.016 0.025 0.038 0.056 0.076 0.096 0.113 0.120 0.113 0.096 0.075 0.056 0.038 0.025 0.020 \n",
            "                                          model/h0/attn/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h0/ln_2/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h0/ln_2/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                             model/h0/mlp/c_fc/w - [  768,  3072,     1], type =    f16 size =     9.00 MB ->     1.41 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.119 0.113 0.097 0.077 0.056 0.039 0.025 0.021 \n",
            "                                             model/h0/mlp/c_fc/b - [ 3072,     1,     1], type =    f32 size =    0.012 MB\n",
            "                                           model/h0/mlp/c_proj/w - [ 3072,   768,     1], type =    f16 size =     9.00 MB ->     1.41 MB | hist: 0.036 0.015 0.024 0.037 0.054 0.076 0.097 0.115 0.124 0.115 0.096 0.075 0.054 0.037 0.024 0.019 \n",
            "                                           model/h0/mlp/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h1/ln_1/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h1/ln_1/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                          model/h1/attn/c_attn/w - [  768,  2304,     1], type =    f16 size =     6.75 MB ->     1.05 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.115 0.122 0.114 0.098 0.075 0.054 0.037 0.024 0.020 \n",
            "                                          model/h1/attn/c_attn/b - [ 2304,     1,     1], type =    f32 size =    0.009 MB\n",
            "                                          model/h1/attn/c_proj/w - [  768,   768,     1], type =    f16 size =     2.25 MB ->     0.35 MB | hist: 0.036 0.015 0.023 0.036 0.053 0.074 0.097 0.117 0.126 0.117 0.097 0.075 0.054 0.037 0.024 0.020 \n",
            "                                          model/h1/attn/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h1/ln_2/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h1/ln_2/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                             model/h1/mlp/c_fc/w - [  768,  3072,     1], type =    f16 size =     9.00 MB ->     1.41 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.075 0.097 0.115 0.123 0.115 0.097 0.075 0.054 0.037 0.024 0.020 \n",
            "                                             model/h1/mlp/c_fc/b - [ 3072,     1,     1], type =    f32 size =    0.012 MB\n",
            "                                           model/h1/mlp/c_proj/w - [ 3072,   768,     1], type =    f16 size =     9.00 MB ->     1.41 MB | hist: 0.036 0.014 0.023 0.035 0.052 0.074 0.098 0.120 0.131 0.120 0.097 0.073 0.052 0.035 0.022 0.019 \n",
            "                                           model/h1/mlp/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h2/ln_1/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h2/ln_1/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                          model/h2/attn/c_attn/w - [  768,  2304,     1], type =    f16 size =     6.75 MB ->     1.05 MB | hist: 0.036 0.014 0.023 0.036 0.054 0.075 0.098 0.117 0.127 0.117 0.099 0.074 0.052 0.036 0.023 0.019 \n",
            "                                          model/h2/attn/c_attn/b - [ 2304,     1,     1], type =    f32 size =    0.009 MB\n",
            "                                          model/h2/attn/c_proj/w - [  768,   768,     1], type =    f16 size =     2.25 MB ->     0.35 MB | hist: 0.036 0.014 0.023 0.035 0.052 0.073 0.097 0.120 0.130 0.120 0.097 0.074 0.053 0.035 0.023 0.019 \n",
            "                                          model/h2/attn/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h2/ln_2/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h2/ln_2/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                             model/h2/mlp/c_fc/w - [  768,  3072,     1], type =    f16 size =     9.00 MB ->     1.41 MB | hist: 0.036 0.015 0.024 0.037 0.054 0.075 0.097 0.115 0.122 0.115 0.098 0.075 0.054 0.037 0.024 0.020 \n",
            "                                             model/h2/mlp/c_fc/b - [ 3072,     1,     1], type =    f32 size =    0.012 MB\n",
            "                                           model/h2/mlp/c_proj/w - [ 3072,   768,     1], type =    f16 size =     9.00 MB ->     1.41 MB | hist: 0.036 0.014 0.022 0.034 0.052 0.074 0.098 0.119 0.134 0.119 0.098 0.074 0.052 0.035 0.022 0.018 \n",
            "                                           model/h2/mlp/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h3/ln_1/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h3/ln_1/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                          model/h3/attn/c_attn/w - [  768,  2304,     1], type =    f16 size =     6.75 MB ->     1.05 MB | hist: 0.036 0.014 0.021 0.034 0.050 0.071 0.097 0.123 0.139 0.124 0.096 0.071 0.049 0.034 0.022 0.018 \n",
            "                                          model/h3/attn/c_attn/b - [ 2304,     1,     1], type =    f32 size =    0.009 MB\n",
            "                                          model/h3/attn/c_proj/w - [  768,   768,     1], type =    f16 size =     2.25 MB ->     0.35 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
            "                                          model/h3/attn/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h3/ln_2/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h3/ln_2/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                             model/h3/mlp/c_fc/w - [  768,  3072,     1], type =    f16 size =     9.00 MB ->     1.41 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.114 0.121 0.115 0.097 0.076 0.055 0.037 0.024 0.020 \n",
            "                                             model/h3/mlp/c_fc/b - [ 3072,     1,     1], type =    f32 size =    0.012 MB\n",
            "                                           model/h3/mlp/c_proj/w - [ 3072,   768,     1], type =    f16 size =     9.00 MB ->     1.41 MB | hist: 0.036 0.014 0.022 0.035 0.052 0.074 0.098 0.120 0.132 0.120 0.099 0.074 0.052 0.034 0.022 0.018 \n",
            "                                           model/h3/mlp/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h4/ln_1/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h4/ln_1/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                          model/h4/attn/c_attn/w - [  768,  2304,     1], type =    f16 size =     6.75 MB ->     1.05 MB | hist: 0.036 0.014 0.022 0.035 0.051 0.073 0.097 0.121 0.133 0.123 0.098 0.072 0.051 0.033 0.022 0.018 \n",
            "                                          model/h4/attn/c_attn/b - [ 2304,     1,     1], type =    f32 size =    0.009 MB\n",
            "                                          model/h4/attn/c_proj/w - [  768,   768,     1], type =    f16 size =     2.25 MB ->     0.35 MB | hist: 0.036 0.016 0.025 0.038 0.056 0.076 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.021 \n",
            "                                          model/h4/attn/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h4/ln_2/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h4/ln_2/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                             model/h4/mlp/c_fc/w - [  768,  3072,     1], type =    f16 size =     9.00 MB ->     1.41 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.113 0.120 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
            "                                             model/h4/mlp/c_fc/b - [ 3072,     1,     1], type =    f32 size =    0.012 MB\n",
            "                                           model/h4/mlp/c_proj/w - [ 3072,   768,     1], type =    f16 size =     9.00 MB ->     1.41 MB | hist: 0.036 0.014 0.023 0.036 0.053 0.075 0.098 0.117 0.126 0.117 0.098 0.075 0.053 0.036 0.023 0.019 \n",
            "                                           model/h4/mlp/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h5/ln_1/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h5/ln_1/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                          model/h5/attn/c_attn/w - [  768,  2304,     1], type =    f16 size =     6.75 MB ->     1.05 MB | hist: 0.036 0.014 0.023 0.035 0.053 0.073 0.097 0.116 0.128 0.120 0.098 0.074 0.054 0.035 0.023 0.020 \n",
            "                                          model/h5/attn/c_attn/b - [ 2304,     1,     1], type =    f32 size =    0.009 MB\n",
            "                                          model/h5/attn/c_proj/w - [  768,   768,     1], type =    f16 size =     2.25 MB ->     0.35 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
            "                                          model/h5/attn/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h5/ln_2/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h5/ln_2/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                             model/h5/mlp/c_fc/w - [  768,  3072,     1], type =    f16 size =     9.00 MB ->     1.41 MB | hist: 0.036 0.015 0.025 0.038 0.055 0.076 0.097 0.113 0.120 0.114 0.097 0.076 0.055 0.038 0.025 0.020 \n",
            "                                             model/h5/mlp/c_fc/b - [ 3072,     1,     1], type =    f32 size =    0.012 MB\n",
            "                                           model/h5/mlp/c_proj/w - [ 3072,   768,     1], type =    f16 size =     9.00 MB ->     1.41 MB | hist: 0.036 0.014 0.023 0.036 0.054 0.076 0.098 0.116 0.124 0.116 0.098 0.076 0.054 0.036 0.024 0.019 \n",
            "                                           model/h5/mlp/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h6/ln_1/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h6/ln_1/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                          model/h6/attn/c_attn/w - [  768,  2304,     1], type =    f16 size =     6.75 MB ->     1.05 MB | hist: 0.036 0.015 0.024 0.037 0.053 0.075 0.098 0.115 0.125 0.117 0.098 0.075 0.052 0.036 0.023 0.019 \n",
            "                                          model/h6/attn/c_attn/b - [ 2304,     1,     1], type =    f32 size =    0.009 MB\n",
            "                                          model/h6/attn/c_proj/w - [  768,   768,     1], type =    f16 size =     2.25 MB ->     0.35 MB | hist: 0.037 0.015 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
            "                                          model/h6/attn/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h6/ln_2/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h6/ln_2/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                             model/h6/mlp/c_fc/w - [  768,  3072,     1], type =    f16 size =     9.00 MB ->     1.41 MB | hist: 0.036 0.015 0.025 0.038 0.055 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.021 \n",
            "                                             model/h6/mlp/c_fc/b - [ 3072,     1,     1], type =    f32 size =    0.012 MB\n",
            "                                           model/h6/mlp/c_proj/w - [ 3072,   768,     1], type =    f16 size =     9.00 MB ->     1.41 MB | hist: 0.036 0.014 0.023 0.037 0.054 0.075 0.098 0.116 0.124 0.116 0.098 0.076 0.054 0.036 0.023 0.019 \n",
            "                                           model/h6/mlp/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h7/ln_1/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h7/ln_1/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                          model/h7/attn/c_attn/w - [  768,  2304,     1], type =    f16 size =     6.75 MB ->     1.05 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.098 0.115 0.121 0.115 0.097 0.075 0.054 0.037 0.024 0.019 \n",
            "                                          model/h7/attn/c_attn/b - [ 2304,     1,     1], type =    f32 size =    0.009 MB\n",
            "                                          model/h7/attn/c_proj/w - [  768,   768,     1], type =    f16 size =     2.25 MB ->     0.35 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.112 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.021 \n",
            "                                          model/h7/attn/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h7/ln_2/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h7/ln_2/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                             model/h7/mlp/c_fc/w - [  768,  3072,     1], type =    f16 size =     9.00 MB ->     1.41 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.113 0.120 0.113 0.097 0.077 0.056 0.038 0.025 0.020 \n",
            "                                             model/h7/mlp/c_fc/b - [ 3072,     1,     1], type =    f32 size =    0.012 MB\n",
            "                                           model/h7/mlp/c_proj/w - [ 3072,   768,     1], type =    f16 size =     9.00 MB ->     1.41 MB | hist: 0.036 0.014 0.023 0.036 0.054 0.075 0.098 0.117 0.124 0.117 0.098 0.076 0.054 0.036 0.023 0.019 \n",
            "                                           model/h7/mlp/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h8/ln_1/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h8/ln_1/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                          model/h8/attn/c_attn/w - [  768,  2304,     1], type =    f16 size =     6.75 MB ->     1.05 MB | hist: 0.037 0.015 0.025 0.038 0.056 0.075 0.096 0.115 0.121 0.113 0.096 0.076 0.055 0.038 0.024 0.021 \n",
            "                                          model/h8/attn/c_attn/b - [ 2304,     1,     1], type =    f32 size =    0.009 MB\n",
            "                                          model/h8/attn/c_proj/w - [  768,   768,     1], type =    f16 size =     2.25 MB ->     0.35 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.111 0.097 0.077 0.056 0.038 0.025 0.021 \n",
            "                                          model/h8/attn/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h8/ln_2/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h8/ln_2/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                             model/h8/mlp/c_fc/w - [  768,  3072,     1], type =    f16 size =     9.00 MB ->     1.41 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
            "                                             model/h8/mlp/c_fc/b - [ 3072,     1,     1], type =    f32 size =    0.012 MB\n",
            "                                           model/h8/mlp/c_proj/w - [ 3072,   768,     1], type =    f16 size =     9.00 MB ->     1.41 MB | hist: 0.036 0.014 0.024 0.037 0.054 0.076 0.098 0.115 0.122 0.115 0.098 0.076 0.055 0.037 0.024 0.019 \n",
            "                                           model/h8/mlp/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h9/ln_1/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h9/ln_1/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                          model/h9/attn/c_attn/w - [  768,  2304,     1], type =    f16 size =     6.75 MB ->     1.05 MB | hist: 0.036 0.014 0.023 0.037 0.054 0.074 0.097 0.117 0.125 0.116 0.097 0.075 0.054 0.036 0.024 0.019 \n",
            "                                          model/h9/attn/c_attn/b - [ 2304,     1,     1], type =    f32 size =    0.009 MB\n",
            "                                          model/h9/attn/c_proj/w - [  768,   768,     1], type =    f16 size =     2.25 MB ->     0.35 MB | hist: 0.036 0.015 0.024 0.038 0.056 0.076 0.098 0.114 0.119 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
            "                                          model/h9/attn/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h9/ln_2/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h9/ln_2/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                             model/h9/mlp/c_fc/w - [  768,  3072,     1], type =    f16 size =     9.00 MB ->     1.41 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.113 0.097 0.077 0.056 0.038 0.025 0.020 \n",
            "                                             model/h9/mlp/c_fc/b - [ 3072,     1,     1], type =    f32 size =    0.012 MB\n",
            "                                           model/h9/mlp/c_proj/w - [ 3072,   768,     1], type =    f16 size =     9.00 MB ->     1.41 MB | hist: 0.036 0.014 0.024 0.037 0.054 0.076 0.098 0.115 0.122 0.116 0.098 0.076 0.055 0.037 0.024 0.019 \n",
            "                                           model/h9/mlp/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h10/ln_1/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h10/ln_1/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                         model/h10/attn/c_attn/w - [  768,  2304,     1], type =    f16 size =     6.75 MB ->     1.05 MB | hist: 0.036 0.014 0.023 0.036 0.053 0.074 0.097 0.117 0.128 0.118 0.097 0.075 0.054 0.036 0.024 0.020 \n",
            "                                         model/h10/attn/c_attn/b - [ 2304,     1,     1], type =    f32 size =    0.009 MB\n",
            "                                         model/h10/attn/c_proj/w - [  768,   768,     1], type =    f16 size =     2.25 MB ->     0.35 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.057 0.039 0.025 0.021 \n",
            "                                         model/h10/attn/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h10/ln_2/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h10/ln_2/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                            model/h10/mlp/c_fc/w - [  768,  3072,     1], type =    f16 size =     9.00 MB ->     1.41 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.021 \n",
            "                                            model/h10/mlp/c_fc/b - [ 3072,     1,     1], type =    f32 size =    0.012 MB\n",
            "                                          model/h10/mlp/c_proj/w - [ 3072,   768,     1], type =    f16 size =     9.00 MB ->     1.41 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.098 0.114 0.121 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
            "                                          model/h10/mlp/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h11/ln_1/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h11/ln_1/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                         model/h11/attn/c_attn/w - [  768,  2304,     1], type =    f16 size =     6.75 MB ->     1.05 MB | hist: 0.036 0.015 0.023 0.037 0.055 0.074 0.097 0.117 0.125 0.117 0.097 0.075 0.054 0.036 0.024 0.019 \n",
            "                                         model/h11/attn/c_attn/b - [ 2304,     1,     1], type =    f32 size =    0.009 MB\n",
            "                                         model/h11/attn/c_proj/w - [  768,   768,     1], type =    f16 size =     2.25 MB ->     0.35 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.097 0.112 0.118 0.112 0.096 0.077 0.056 0.038 0.025 0.020 \n",
            "                                         model/h11/attn/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h11/ln_2/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h11/ln_2/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                            model/h11/mlp/c_fc/w - [  768,  3072,     1], type =    f16 size =     9.00 MB ->     1.41 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
            "                                            model/h11/mlp/c_fc/b - [ 3072,     1,     1], type =    f32 size =    0.012 MB\n",
            "                                          model/h11/mlp/c_proj/w - [ 3072,   768,     1], type =    f16 size =     9.00 MB ->     1.41 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.098 0.115 0.122 0.114 0.098 0.076 0.055 0.037 0.024 0.020 \n",
            "                                          model/h11/mlp/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h12/ln_1/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h12/ln_1/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                         model/h12/attn/c_attn/w - [  768,  2304,     1], type =    f16 size =     6.75 MB ->     1.05 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.096 0.115 0.121 0.114 0.098 0.076 0.055 0.037 0.024 0.020 \n",
            "                                         model/h12/attn/c_attn/b - [ 2304,     1,     1], type =    f32 size =    0.009 MB\n",
            "                                         model/h12/attn/c_proj/w - [  768,   768,     1], type =    f16 size =     2.25 MB ->     0.35 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.077 0.097 0.113 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.021 \n",
            "                                         model/h12/attn/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h12/ln_2/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h12/ln_2/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                            model/h12/mlp/c_fc/w - [  768,  3072,     1], type =    f16 size =     9.00 MB ->     1.41 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
            "                                            model/h12/mlp/c_fc/b - [ 3072,     1,     1], type =    f32 size =    0.012 MB\n",
            "                                          model/h12/mlp/c_proj/w - [ 3072,   768,     1], type =    f16 size =     9.00 MB ->     1.41 MB | hist: 0.036 0.014 0.024 0.037 0.055 0.076 0.098 0.115 0.122 0.115 0.098 0.076 0.055 0.037 0.024 0.019 \n",
            "                                          model/h12/mlp/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h13/ln_1/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h13/ln_1/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                         model/h13/attn/c_attn/w - [  768,  2304,     1], type =    f16 size =     6.75 MB ->     1.05 MB | hist: 0.036 0.014 0.024 0.037 0.055 0.075 0.097 0.115 0.121 0.115 0.096 0.077 0.054 0.038 0.024 0.020 \n",
            "                                         model/h13/attn/c_attn/b - [ 2304,     1,     1], type =    f32 size =    0.009 MB\n",
            "                                         model/h13/attn/c_proj/w - [  768,   768,     1], type =    f16 size =     2.25 MB ->     0.35 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.076 0.057 0.038 0.025 0.021 \n",
            "                                         model/h13/attn/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h13/ln_2/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h13/ln_2/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                            model/h13/mlp/c_fc/w - [  768,  3072,     1], type =    f16 size =     9.00 MB ->     1.41 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
            "                                            model/h13/mlp/c_fc/b - [ 3072,     1,     1], type =    f32 size =    0.012 MB\n",
            "                                          model/h13/mlp/c_proj/w - [ 3072,   768,     1], type =    f16 size =     9.00 MB ->     1.41 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.120 0.114 0.098 0.076 0.055 0.038 0.024 0.020 \n",
            "                                          model/h13/mlp/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h14/ln_1/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h14/ln_1/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                         model/h14/attn/c_attn/w - [  768,  2304,     1], type =    f16 size =     6.75 MB ->     1.05 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.114 0.121 0.113 0.097 0.076 0.056 0.038 0.024 0.020 \n",
            "                                         model/h14/attn/c_attn/b - [ 2304,     1,     1], type =    f32 size =    0.009 MB\n",
            "                                         model/h14/attn/c_proj/w - [  768,   768,     1], type =    f16 size =     2.25 MB ->     0.35 MB | hist: 0.036 0.016 0.025 0.038 0.056 0.077 0.097 0.112 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
            "                                         model/h14/attn/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h14/ln_2/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h14/ln_2/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                            model/h14/mlp/c_fc/w - [  768,  3072,     1], type =    f16 size =     9.00 MB ->     1.41 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
            "                                            model/h14/mlp/c_fc/b - [ 3072,     1,     1], type =    f32 size =    0.012 MB\n",
            "                                          model/h14/mlp/c_proj/w - [ 3072,   768,     1], type =    f16 size =     9.00 MB ->     1.41 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.098 0.115 0.122 0.115 0.098 0.076 0.055 0.037 0.024 0.020 \n",
            "                                          model/h14/mlp/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h15/ln_1/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h15/ln_1/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                         model/h15/attn/c_attn/w - [  768,  2304,     1], type =    f16 size =     6.75 MB ->     1.05 MB | hist: 0.036 0.015 0.024 0.037 0.054 0.076 0.097 0.114 0.122 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
            "                                         model/h15/attn/c_attn/b - [ 2304,     1,     1], type =    f32 size =    0.009 MB\n",
            "                                         model/h15/attn/c_proj/w - [  768,   768,     1], type =    f16 size =     2.25 MB ->     0.35 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.096 0.113 0.119 0.114 0.098 0.076 0.056 0.038 0.025 0.020 \n",
            "                                         model/h15/attn/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h15/ln_2/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h15/ln_2/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                            model/h15/mlp/c_fc/w - [  768,  3072,     1], type =    f16 size =     9.00 MB ->     1.41 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.111 0.117 0.112 0.097 0.077 0.057 0.039 0.025 0.021 \n",
            "                                            model/h15/mlp/c_fc/b - [ 3072,     1,     1], type =    f32 size =    0.012 MB\n",
            "                                          model/h15/mlp/c_proj/w - [ 3072,   768,     1], type =    f16 size =     9.00 MB ->     1.41 MB | hist: 0.036 0.014 0.024 0.037 0.055 0.076 0.098 0.115 0.121 0.115 0.098 0.076 0.055 0.037 0.024 0.019 \n",
            "                                          model/h15/mlp/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h16/ln_1/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h16/ln_1/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                         model/h16/attn/c_attn/w - [  768,  2304,     1], type =    f16 size =     6.75 MB ->     1.05 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.074 0.097 0.115 0.122 0.114 0.096 0.076 0.055 0.038 0.024 0.021 \n",
            "                                         model/h16/attn/c_attn/b - [ 2304,     1,     1], type =    f32 size =    0.009 MB\n",
            "                                         model/h16/attn/c_proj/w - [  768,   768,     1], type =    f16 size =     2.25 MB ->     0.35 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.077 0.097 0.114 0.119 0.114 0.097 0.076 0.055 0.038 0.025 0.020 \n",
            "                                         model/h16/attn/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h16/ln_2/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h16/ln_2/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                            model/h16/mlp/c_fc/w - [  768,  3072,     1], type =    f16 size =     9.00 MB ->     1.41 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
            "                                            model/h16/mlp/c_fc/b - [ 3072,     1,     1], type =    f32 size =    0.012 MB\n",
            "                                          model/h16/mlp/c_proj/w - [ 3072,   768,     1], type =    f16 size =     9.00 MB ->     1.41 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.098 0.114 0.121 0.115 0.098 0.076 0.055 0.037 0.024 0.020 \n",
            "                                          model/h16/mlp/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h17/ln_1/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h17/ln_1/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                         model/h17/attn/c_attn/w - [  768,  2304,     1], type =    f16 size =     6.75 MB ->     1.05 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.075 0.097 0.114 0.122 0.114 0.098 0.076 0.055 0.038 0.024 0.020 \n",
            "                                         model/h17/attn/c_attn/b - [ 2304,     1,     1], type =    f32 size =    0.009 MB\n",
            "                                         model/h17/attn/c_proj/w - [  768,   768,     1], type =    f16 size =     2.25 MB ->     0.35 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.113 0.118 0.113 0.096 0.077 0.056 0.038 0.024 0.020 \n",
            "                                         model/h17/attn/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h17/ln_2/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h17/ln_2/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                            model/h17/mlp/c_fc/w - [  768,  3072,     1], type =    f16 size =     9.00 MB ->     1.41 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.057 0.039 0.025 0.021 \n",
            "                                            model/h17/mlp/c_fc/b - [ 3072,     1,     1], type =    f32 size =    0.012 MB\n",
            "                                          model/h17/mlp/c_proj/w - [ 3072,   768,     1], type =    f16 size =     9.00 MB ->     1.41 MB | hist: 0.036 0.015 0.024 0.037 0.054 0.076 0.098 0.115 0.122 0.115 0.098 0.076 0.054 0.037 0.023 0.019 \n",
            "                                          model/h17/mlp/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h18/ln_1/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h18/ln_1/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                         model/h18/attn/c_attn/w - [  768,  2304,     1], type =    f16 size =     6.75 MB ->     1.05 MB | hist: 0.036 0.015 0.025 0.038 0.055 0.077 0.095 0.112 0.122 0.115 0.096 0.076 0.056 0.037 0.025 0.020 \n",
            "                                         model/h18/attn/c_attn/b - [ 2304,     1,     1], type =    f32 size =    0.009 MB\n",
            "                                         model/h18/attn/c_proj/w - [  768,   768,     1], type =    f16 size =     2.25 MB ->     0.35 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.116 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
            "                                         model/h18/attn/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h18/ln_2/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h18/ln_2/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                            model/h18/mlp/c_fc/w - [  768,  3072,     1], type =    f16 size =     9.00 MB ->     1.41 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.112 0.097 0.077 0.057 0.039 0.025 0.021 \n",
            "                                            model/h18/mlp/c_fc/b - [ 3072,     1,     1], type =    f32 size =    0.012 MB\n",
            "                                          model/h18/mlp/c_proj/w - [ 3072,   768,     1], type =    f16 size =     9.00 MB ->     1.41 MB | hist: 0.036 0.014 0.023 0.036 0.054 0.075 0.099 0.117 0.124 0.117 0.099 0.075 0.053 0.036 0.023 0.019 \n",
            "                                          model/h18/mlp/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h19/ln_1/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h19/ln_1/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                         model/h19/attn/c_attn/w - [  768,  2304,     1], type =    f16 size =     6.75 MB ->     1.05 MB | hist: 0.036 0.015 0.025 0.038 0.055 0.076 0.096 0.113 0.120 0.113 0.097 0.076 0.055 0.038 0.025 0.021 \n",
            "                                         model/h19/attn/c_attn/b - [ 2304,     1,     1], type =    f32 size =    0.009 MB\n",
            "                                         model/h19/attn/c_proj/w - [  768,   768,     1], type =    f16 size =     2.25 MB ->     0.35 MB | hist: 0.036 0.016 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.111 0.097 0.077 0.056 0.038 0.025 0.021 \n",
            "                                         model/h19/attn/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h19/ln_2/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h19/ln_2/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                            model/h19/mlp/c_fc/w - [  768,  3072,     1], type =    f16 size =     9.00 MB ->     1.41 MB | hist: 0.037 0.015 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
            "                                            model/h19/mlp/c_fc/b - [ 3072,     1,     1], type =    f32 size =    0.012 MB\n",
            "                                          model/h19/mlp/c_proj/w - [ 3072,   768,     1], type =    f16 size =     9.00 MB ->     1.41 MB | hist: 0.036 0.014 0.023 0.035 0.053 0.075 0.098 0.118 0.126 0.118 0.099 0.076 0.053 0.035 0.022 0.018 \n",
            "                                          model/h19/mlp/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                    model/ln_f/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                    model/ln_f/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                   model/lm_head - [  768, 49152,     1], type =    f16 size =   144.00 MB ->    22.50 MB | hist: 0.036 0.013 0.021 0.033 0.050 0.073 0.099 0.123 0.132 0.123 0.100 0.074 0.051 0.034 0.021 0.018 \n",
            "ggml_common_quantize_0: model size  =   852.77 MB\n",
            "ggml_common_quantize_0: quant size  =   154.14 MB | ftype = 2 (q4_0)\n",
            "ggml_common_quantize_0: hist: 0.036 0.014 0.023 0.036 0.053 0.075 0.098 0.117 0.126 0.117 0.098 0.075 0.054 0.036 0.023 0.019 \n",
            "\n",
            "main: quantize time =  1661.65 ms\n",
            "main:    total time =  1661.65 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd starcoder.cpp && ./quantize ../models/rahuldshetty/tiny-starcoder-instruct-ggml.bin ../quantized/tiny-starcoder-instruct-ggml-q5_0.bin 8"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DPN15mJ_VR24",
        "outputId": "78ae0a62-b75c-4a4d-f1fe-e5c122d8495c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "starcoder_model_quantize: loading model from '../models/rahuldshetty/tiny-starcoder-instruct-ggml.bin'\n",
            "starcoder_model_quantize: n_vocab     = 49152\n",
            "starcoder_model_quantize: n_ctx       = 8192\n",
            "starcoder_model_quantize: n_embd      = 768\n",
            "starcoder_model_quantize: n_head      = 12\n",
            "starcoder_model_quantize: n_layer     = 20\n",
            "starcoder_model_quantize: ftype (src) = 1\n",
            "starcoder_model_quantize: qntvr (src) = 0\n",
            "starcoder_model_quantize: ftype (dst) = 1008\n",
            "starcoder_model_quantize: qntvr (dst) = 1\n",
            "                                                       model/wte - [  768, 49152,     1], type =    f16 size =   144.00 MB ->    24.75 MB | hist: 0.083 0.064 0.057 0.053 0.052 0.058 0.066 0.075 0.087 0.067 0.059 0.052 0.050 0.052 0.059 0.066 \n",
            "                                                       model/wpe - [  768,  8192,     1], type =    f32 size =   24.000 MB\n",
            "                                                 model/h0/ln_1/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h0/ln_1/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                          model/h0/attn/c_attn/w - [  768,  2304,     1], type =    f16 size =     6.75 MB ->     1.16 MB | hist: 0.079 0.064 0.060 0.058 0.059 0.061 0.066 0.072 0.079 0.064 0.059 0.054 0.053 0.053 0.057 0.060 \n",
            "                                          model/h0/attn/c_attn/b - [ 2304,     1,     1], type =    f32 size =    0.009 MB\n",
            "                                          model/h0/attn/c_proj/w - [  768,   768,     1], type =    f16 size =     2.25 MB ->     0.39 MB | hist: 0.081 0.064 0.060 0.057 0.059 0.062 0.067 0.072 0.078 0.063 0.060 0.055 0.053 0.054 0.056 0.059 \n",
            "                                          model/h0/attn/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h0/ln_2/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h0/ln_2/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                             model/h0/mlp/c_fc/w - [  768,  3072,     1], type =    f16 size =     9.00 MB ->     1.55 MB | hist: 0.079 0.063 0.060 0.058 0.058 0.062 0.066 0.071 0.079 0.064 0.060 0.055 0.054 0.054 0.057 0.060 \n",
            "                                             model/h0/mlp/c_fc/b - [ 3072,     1,     1], type =    f32 size =    0.012 MB\n",
            "                                           model/h0/mlp/c_proj/w - [ 3072,   768,     1], type =    f16 size =     9.00 MB ->     1.55 MB | hist: 0.081 0.063 0.059 0.057 0.057 0.061 0.067 0.073 0.081 0.064 0.059 0.054 0.052 0.053 0.057 0.061 \n",
            "                                           model/h0/mlp/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h1/ln_1/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h1/ln_1/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                          model/h1/attn/c_attn/w - [  768,  2304,     1], type =    f16 size =     6.75 MB ->     1.16 MB | hist: 0.080 0.063 0.060 0.057 0.058 0.061 0.066 0.072 0.081 0.064 0.060 0.053 0.052 0.054 0.057 0.061 \n",
            "                                          model/h1/attn/c_attn/b - [ 2304,     1,     1], type =    f32 size =    0.009 MB\n",
            "                                          model/h1/attn/c_proj/w - [  768,   768,     1], type =    f16 size =     2.25 MB ->     0.39 MB | hist: 0.082 0.064 0.059 0.056 0.056 0.060 0.066 0.073 0.083 0.066 0.058 0.054 0.051 0.053 0.057 0.063 \n",
            "                                          model/h1/attn/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h1/ln_2/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h1/ln_2/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                             model/h1/mlp/c_fc/w - [  768,  3072,     1], type =    f16 size =     9.00 MB ->     1.55 MB | hist: 0.081 0.063 0.060 0.057 0.057 0.061 0.067 0.072 0.081 0.064 0.059 0.054 0.052 0.054 0.057 0.061 \n",
            "                                             model/h1/mlp/c_fc/b - [ 3072,     1,     1], type =    f32 size =    0.012 MB\n",
            "                                           model/h1/mlp/c_proj/w - [ 3072,   768,     1], type =    f16 size =     9.00 MB ->     1.55 MB | hist: 0.084 0.065 0.058 0.054 0.054 0.059 0.067 0.075 0.085 0.066 0.058 0.052 0.050 0.052 0.057 0.063 \n",
            "                                           model/h1/mlp/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h2/ln_1/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h2/ln_1/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                          model/h2/attn/c_attn/w - [  768,  2304,     1], type =    f16 size =     6.75 MB ->     1.16 MB | hist: 0.082 0.065 0.060 0.056 0.056 0.060 0.068 0.074 0.082 0.065 0.059 0.052 0.051 0.052 0.057 0.062 \n",
            "                                          model/h2/attn/c_attn/b - [ 2304,     1,     1], type =    f32 size =    0.009 MB\n",
            "                                          model/h2/attn/c_proj/w - [  768,   768,     1], type =    f16 size =     2.25 MB ->     0.39 MB | hist: 0.083 0.065 0.058 0.055 0.054 0.059 0.066 0.075 0.086 0.066 0.059 0.052 0.051 0.052 0.057 0.063 \n",
            "                                          model/h2/attn/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h2/ln_2/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h2/ln_2/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                             model/h2/mlp/c_fc/w - [  768,  3072,     1], type =    f16 size =     9.00 MB ->     1.55 MB | hist: 0.080 0.063 0.060 0.057 0.057 0.061 0.066 0.073 0.081 0.065 0.059 0.054 0.052 0.053 0.057 0.061 \n",
            "                                             model/h2/mlp/c_fc/b - [ 3072,     1,     1], type =    f32 size =    0.012 MB\n",
            "                                           model/h2/mlp/c_proj/w - [ 3072,   768,     1], type =    f16 size =     9.00 MB ->     1.55 MB | hist: 0.084 0.064 0.058 0.054 0.054 0.059 0.066 0.075 0.087 0.066 0.058 0.052 0.050 0.052 0.057 0.064 \n",
            "                                           model/h2/mlp/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h3/ln_1/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h3/ln_1/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                          model/h3/attn/c_attn/w - [  768,  2304,     1], type =    f16 size =     6.75 MB ->     1.16 MB | hist: 0.086 0.065 0.057 0.052 0.052 0.057 0.067 0.078 0.090 0.068 0.057 0.051 0.047 0.051 0.056 0.065 \n",
            "                                          model/h3/attn/c_attn/b - [ 2304,     1,     1], type =    f32 size =    0.009 MB\n",
            "                                          model/h3/attn/c_proj/w - [  768,   768,     1], type =    f16 size =     2.25 MB ->     0.39 MB | hist: 0.079 0.063 0.059 0.058 0.058 0.062 0.066 0.071 0.078 0.064 0.060 0.055 0.054 0.055 0.057 0.060 \n",
            "                                          model/h3/attn/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h3/ln_2/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h3/ln_2/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                             model/h3/mlp/c_fc/w - [  768,  3072,     1], type =    f16 size =     9.00 MB ->     1.55 MB | hist: 0.080 0.063 0.060 0.057 0.057 0.061 0.067 0.072 0.080 0.065 0.059 0.055 0.052 0.054 0.057 0.061 \n",
            "                                             model/h3/mlp/c_fc/b - [ 3072,     1,     1], type =    f32 size =    0.012 MB\n",
            "                                           model/h3/mlp/c_proj/w - [ 3072,   768,     1], type =    f16 size =     9.00 MB ->     1.55 MB | hist: 0.084 0.064 0.059 0.054 0.054 0.059 0.066 0.075 0.086 0.066 0.059 0.052 0.050 0.052 0.057 0.064 \n",
            "                                           model/h3/mlp/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h4/ln_1/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h4/ln_1/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                          model/h4/attn/c_attn/w - [  768,  2304,     1], type =    f16 size =     6.75 MB ->     1.16 MB | hist: 0.085 0.065 0.059 0.054 0.054 0.059 0.066 0.076 0.087 0.067 0.059 0.051 0.048 0.051 0.056 0.065 \n",
            "                                          model/h4/attn/c_attn/b - [ 2304,     1,     1], type =    f32 size =    0.009 MB\n",
            "                                          model/h4/attn/c_proj/w - [  768,   768,     1], type =    f16 size =     2.25 MB ->     0.39 MB | hist: 0.078 0.063 0.060 0.058 0.058 0.062 0.067 0.071 0.079 0.064 0.060 0.056 0.054 0.054 0.057 0.060 \n",
            "                                          model/h4/attn/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h4/ln_2/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h4/ln_2/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                             model/h4/mlp/c_fc/w - [  768,  3072,     1], type =    f16 size =     9.00 MB ->     1.55 MB | hist: 0.079 0.063 0.060 0.057 0.058 0.061 0.067 0.071 0.080 0.064 0.059 0.055 0.053 0.054 0.057 0.060 \n",
            "                                             model/h4/mlp/c_fc/b - [ 3072,     1,     1], type =    f32 size =    0.012 MB\n",
            "                                           model/h4/mlp/c_proj/w - [ 3072,   768,     1], type =    f16 size =     9.00 MB ->     1.55 MB | hist: 0.082 0.064 0.059 0.056 0.056 0.060 0.067 0.074 0.082 0.065 0.059 0.053 0.051 0.053 0.057 0.062 \n",
            "                                           model/h4/mlp/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h5/ln_1/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h5/ln_1/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                          model/h5/attn/c_attn/w - [  768,  2304,     1], type =    f16 size =     6.75 MB ->     1.16 MB | hist: 0.082 0.064 0.059 0.055 0.055 0.060 0.066 0.073 0.084 0.067 0.058 0.053 0.051 0.052 0.057 0.063 \n",
            "                                          model/h5/attn/c_attn/b - [ 2304,     1,     1], type =    f32 size =    0.009 MB\n",
            "                                          model/h5/attn/c_proj/w - [  768,   768,     1], type =    f16 size =     2.25 MB ->     0.39 MB | hist: 0.078 0.063 0.060 0.058 0.059 0.062 0.067 0.071 0.078 0.063 0.060 0.056 0.053 0.054 0.057 0.060 \n",
            "                                          model/h5/attn/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h5/ln_2/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h5/ln_2/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                             model/h5/mlp/c_fc/w - [  768,  3072,     1], type =    f16 size =     9.00 MB ->     1.55 MB | hist: 0.079 0.063 0.060 0.058 0.058 0.061 0.067 0.071 0.080 0.065 0.060 0.055 0.053 0.054 0.057 0.061 \n",
            "                                             model/h5/mlp/c_fc/b - [ 3072,     1,     1], type =    f32 size =    0.012 MB\n",
            "                                           model/h5/mlp/c_proj/w - [ 3072,   768,     1], type =    f16 size =     9.00 MB ->     1.55 MB | hist: 0.081 0.063 0.060 0.056 0.057 0.061 0.067 0.073 0.082 0.065 0.059 0.054 0.052 0.053 0.057 0.061 \n",
            "                                           model/h5/mlp/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h6/ln_1/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h6/ln_1/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                          model/h6/attn/c_attn/w - [  768,  2304,     1], type =    f16 size =     6.75 MB ->     1.16 MB | hist: 0.082 0.063 0.060 0.057 0.055 0.060 0.067 0.073 0.082 0.066 0.059 0.053 0.051 0.054 0.057 0.061 \n",
            "                                          model/h6/attn/c_attn/b - [ 2304,     1,     1], type =    f32 size =    0.009 MB\n",
            "                                          model/h6/attn/c_proj/w - [  768,   768,     1], type =    f16 size =     2.25 MB ->     0.39 MB | hist: 0.078 0.063 0.060 0.059 0.059 0.063 0.066 0.071 0.078 0.063 0.060 0.055 0.054 0.054 0.057 0.059 \n",
            "                                          model/h6/attn/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h6/ln_2/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h6/ln_2/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                             model/h6/mlp/c_fc/w - [  768,  3072,     1], type =    f16 size =     9.00 MB ->     1.55 MB | hist: 0.079 0.063 0.060 0.058 0.058 0.061 0.066 0.071 0.079 0.064 0.060 0.055 0.053 0.054 0.057 0.061 \n",
            "                                             model/h6/mlp/c_fc/b - [ 3072,     1,     1], type =    f32 size =    0.012 MB\n",
            "                                           model/h6/mlp/c_proj/w - [ 3072,   768,     1], type =    f16 size =     9.00 MB ->     1.55 MB | hist: 0.081 0.064 0.059 0.056 0.057 0.061 0.067 0.072 0.081 0.065 0.059 0.054 0.052 0.053 0.057 0.062 \n",
            "                                           model/h6/mlp/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h7/ln_1/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h7/ln_1/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                          model/h7/attn/c_attn/w - [  768,  2304,     1], type =    f16 size =     6.75 MB ->     1.16 MB | hist: 0.081 0.064 0.060 0.057 0.057 0.062 0.067 0.071 0.080 0.064 0.058 0.054 0.052 0.054 0.056 0.061 \n",
            "                                          model/h7/attn/c_attn/b - [ 2304,     1,     1], type =    f32 size =    0.009 MB\n",
            "                                          model/h7/attn/c_proj/w - [  768,   768,     1], type =    f16 size =     2.25 MB ->     0.39 MB | hist: 0.078 0.063 0.060 0.057 0.058 0.062 0.067 0.071 0.080 0.065 0.060 0.055 0.053 0.054 0.057 0.060 \n",
            "                                          model/h7/attn/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h7/ln_2/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h7/ln_2/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                             model/h7/mlp/c_fc/w - [  768,  3072,     1], type =    f16 size =     9.00 MB ->     1.55 MB | hist: 0.079 0.063 0.060 0.058 0.058 0.061 0.066 0.071 0.080 0.064 0.060 0.055 0.053 0.054 0.057 0.060 \n",
            "                                             model/h7/mlp/c_fc/b - [ 3072,     1,     1], type =    f32 size =    0.012 MB\n",
            "                                           model/h7/mlp/c_proj/w - [ 3072,   768,     1], type =    f16 size =     9.00 MB ->     1.55 MB | hist: 0.081 0.064 0.059 0.056 0.056 0.060 0.067 0.073 0.082 0.065 0.059 0.054 0.052 0.053 0.057 0.062 \n",
            "                                           model/h7/mlp/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h8/ln_1/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h8/ln_1/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                          model/h8/attn/c_attn/w - [  768,  2304,     1], type =    f16 size =     6.75 MB ->     1.16 MB | hist: 0.080 0.064 0.060 0.058 0.058 0.062 0.066 0.072 0.080 0.064 0.059 0.054 0.052 0.053 0.056 0.061 \n",
            "                                          model/h8/attn/c_attn/b - [ 2304,     1,     1], type =    f32 size =    0.009 MB\n",
            "                                          model/h8/attn/c_proj/w - [  768,   768,     1], type =    f16 size =     2.25 MB ->     0.39 MB | hist: 0.079 0.062 0.061 0.059 0.059 0.062 0.066 0.071 0.078 0.063 0.060 0.056 0.054 0.054 0.057 0.060 \n",
            "                                          model/h8/attn/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h8/ln_2/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h8/ln_2/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                             model/h8/mlp/c_fc/w - [  768,  3072,     1], type =    f16 size =     9.00 MB ->     1.55 MB | hist: 0.079 0.063 0.060 0.058 0.059 0.062 0.067 0.071 0.079 0.064 0.060 0.055 0.053 0.054 0.057 0.060 \n",
            "                                             model/h8/mlp/c_fc/b - [ 3072,     1,     1], type =    f32 size =    0.012 MB\n",
            "                                           model/h8/mlp/c_proj/w - [ 3072,   768,     1], type =    f16 size =     9.00 MB ->     1.55 MB | hist: 0.080 0.063 0.060 0.057 0.057 0.061 0.067 0.073 0.080 0.064 0.059 0.054 0.052 0.053 0.057 0.061 \n",
            "                                           model/h8/mlp/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h9/ln_1/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h9/ln_1/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                          model/h9/attn/c_attn/w - [  768,  2304,     1], type =    f16 size =     6.75 MB ->     1.16 MB | hist: 0.081 0.063 0.059 0.056 0.057 0.060 0.067 0.073 0.083 0.065 0.059 0.054 0.052 0.053 0.057 0.062 \n",
            "                                          model/h9/attn/c_attn/b - [ 2304,     1,     1], type =    f32 size =    0.009 MB\n",
            "                                          model/h9/attn/c_proj/w - [  768,   768,     1], type =    f16 size =     2.25 MB ->     0.39 MB | hist: 0.079 0.063 0.060 0.058 0.058 0.062 0.067 0.072 0.079 0.064 0.059 0.055 0.053 0.054 0.057 0.060 \n",
            "                                          model/h9/attn/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h9/ln_2/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h9/ln_2/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                             model/h9/mlp/c_fc/w - [  768,  3072,     1], type =    f16 size =     9.00 MB ->     1.55 MB | hist: 0.079 0.063 0.060 0.058 0.058 0.062 0.067 0.071 0.079 0.064 0.060 0.056 0.053 0.055 0.057 0.060 \n",
            "                                             model/h9/mlp/c_fc/b - [ 3072,     1,     1], type =    f32 size =    0.012 MB\n",
            "                                           model/h9/mlp/c_proj/w - [ 3072,   768,     1], type =    f16 size =     9.00 MB ->     1.55 MB | hist: 0.080 0.063 0.060 0.057 0.057 0.061 0.067 0.072 0.081 0.065 0.059 0.054 0.052 0.054 0.057 0.061 \n",
            "                                           model/h9/mlp/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h10/ln_1/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h10/ln_1/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                         model/h10/attn/c_attn/w - [  768,  2304,     1], type =    f16 size =     6.75 MB ->     1.16 MB | hist: 0.082 0.063 0.059 0.055 0.056 0.059 0.066 0.072 0.084 0.066 0.058 0.054 0.051 0.053 0.057 0.064 \n",
            "                                         model/h10/attn/c_attn/b - [ 2304,     1,     1], type =    f32 size =    0.009 MB\n",
            "                                         model/h10/attn/c_proj/w - [  768,   768,     1], type =    f16 size =     2.25 MB ->     0.39 MB | hist: 0.079 0.063 0.060 0.058 0.059 0.062 0.067 0.071 0.079 0.064 0.060 0.055 0.053 0.054 0.057 0.059 \n",
            "                                         model/h10/attn/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h10/ln_2/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h10/ln_2/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                            model/h10/mlp/c_fc/w - [  768,  3072,     1], type =    f16 size =     9.00 MB ->     1.55 MB | hist: 0.079 0.063 0.060 0.058 0.059 0.062 0.067 0.071 0.079 0.064 0.060 0.055 0.054 0.054 0.057 0.060 \n",
            "                                            model/h10/mlp/c_fc/b - [ 3072,     1,     1], type =    f32 size =    0.012 MB\n",
            "                                          model/h10/mlp/c_proj/w - [ 3072,   768,     1], type =    f16 size =     9.00 MB ->     1.55 MB | hist: 0.080 0.063 0.060 0.057 0.058 0.061 0.067 0.072 0.080 0.064 0.059 0.055 0.053 0.054 0.057 0.060 \n",
            "                                          model/h10/mlp/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h11/ln_1/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h11/ln_1/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                         model/h11/attn/c_attn/w - [  768,  2304,     1], type =    f16 size =     6.75 MB ->     1.16 MB | hist: 0.081 0.064 0.059 0.056 0.057 0.059 0.067 0.073 0.082 0.065 0.058 0.054 0.051 0.053 0.057 0.062 \n",
            "                                         model/h11/attn/c_attn/b - [ 2304,     1,     1], type =    f32 size =    0.009 MB\n",
            "                                         model/h11/attn/c_proj/w - [  768,   768,     1], type =    f16 size =     2.25 MB ->     0.39 MB | hist: 0.079 0.064 0.060 0.058 0.059 0.062 0.067 0.071 0.078 0.063 0.059 0.056 0.054 0.054 0.057 0.059 \n",
            "                                         model/h11/attn/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h11/ln_2/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h11/ln_2/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                            model/h11/mlp/c_fc/w - [  768,  3072,     1], type =    f16 size =     9.00 MB ->     1.55 MB | hist: 0.078 0.063 0.060 0.058 0.059 0.062 0.066 0.071 0.079 0.064 0.060 0.056 0.054 0.055 0.057 0.060 \n",
            "                                            model/h11/mlp/c_fc/b - [ 3072,     1,     1], type =    f32 size =    0.012 MB\n",
            "                                          model/h11/mlp/c_proj/w - [ 3072,   768,     1], type =    f16 size =     9.00 MB ->     1.55 MB | hist: 0.080 0.063 0.060 0.057 0.057 0.061 0.067 0.072 0.080 0.064 0.059 0.054 0.052 0.054 0.057 0.061 \n",
            "                                          model/h11/mlp/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h12/ln_1/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h12/ln_1/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                         model/h12/attn/c_attn/w - [  768,  2304,     1], type =    f16 size =     6.75 MB ->     1.16 MB | hist: 0.079 0.063 0.061 0.057 0.057 0.061 0.066 0.073 0.080 0.064 0.059 0.055 0.053 0.054 0.056 0.061 \n",
            "                                         model/h12/attn/c_attn/b - [ 2304,     1,     1], type =    f32 size =    0.009 MB\n",
            "                                         model/h12/attn/c_proj/w - [  768,   768,     1], type =    f16 size =     2.25 MB ->     0.39 MB | hist: 0.079 0.063 0.060 0.058 0.058 0.062 0.067 0.072 0.079 0.064 0.059 0.055 0.053 0.055 0.057 0.059 \n",
            "                                         model/h12/attn/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h12/ln_2/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h12/ln_2/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                            model/h12/mlp/c_fc/w - [  768,  3072,     1], type =    f16 size =     9.00 MB ->     1.55 MB | hist: 0.078 0.063 0.060 0.058 0.059 0.062 0.067 0.070 0.078 0.064 0.060 0.056 0.054 0.055 0.057 0.060 \n",
            "                                            model/h12/mlp/c_fc/b - [ 3072,     1,     1], type =    f32 size =    0.012 MB\n",
            "                                          model/h12/mlp/c_proj/w - [ 3072,   768,     1], type =    f16 size =     9.00 MB ->     1.55 MB | hist: 0.080 0.063 0.060 0.057 0.057 0.061 0.067 0.072 0.081 0.065 0.059 0.055 0.052 0.054 0.057 0.061 \n",
            "                                          model/h12/mlp/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h13/ln_1/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h13/ln_1/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                         model/h13/attn/c_attn/w - [  768,  2304,     1], type =    f16 size =     6.75 MB ->     1.16 MB | hist: 0.080 0.063 0.060 0.057 0.057 0.061 0.067 0.072 0.081 0.064 0.059 0.055 0.052 0.054 0.057 0.061 \n",
            "                                         model/h13/attn/c_attn/b - [ 2304,     1,     1], type =    f32 size =    0.009 MB\n",
            "                                         model/h13/attn/c_proj/w - [  768,   768,     1], type =    f16 size =     2.25 MB ->     0.39 MB | hist: 0.079 0.062 0.060 0.058 0.058 0.062 0.066 0.070 0.078 0.064 0.060 0.056 0.054 0.055 0.057 0.061 \n",
            "                                         model/h13/attn/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h13/ln_2/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h13/ln_2/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                            model/h13/mlp/c_fc/w - [  768,  3072,     1], type =    f16 size =     9.00 MB ->     1.55 MB | hist: 0.079 0.062 0.060 0.058 0.059 0.062 0.066 0.070 0.078 0.064 0.060 0.056 0.054 0.054 0.057 0.060 \n",
            "                                            model/h13/mlp/c_fc/b - [ 3072,     1,     1], type =    f32 size =    0.012 MB\n",
            "                                          model/h13/mlp/c_proj/w - [ 3072,   768,     1], type =    f16 size =     9.00 MB ->     1.55 MB | hist: 0.079 0.063 0.060 0.058 0.058 0.061 0.067 0.072 0.079 0.064 0.060 0.055 0.053 0.054 0.057 0.060 \n",
            "                                          model/h13/mlp/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h14/ln_1/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h14/ln_1/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                         model/h14/attn/c_attn/w - [  768,  2304,     1], type =    f16 size =     6.75 MB ->     1.16 MB | hist: 0.079 0.063 0.060 0.057 0.058 0.061 0.066 0.072 0.080 0.065 0.060 0.055 0.053 0.054 0.057 0.060 \n",
            "                                         model/h14/attn/c_attn/b - [ 2304,     1,     1], type =    f32 size =    0.009 MB\n",
            "                                         model/h14/attn/c_proj/w - [  768,   768,     1], type =    f16 size =     2.25 MB ->     0.39 MB | hist: 0.078 0.063 0.060 0.059 0.059 0.063 0.067 0.071 0.078 0.063 0.059 0.055 0.054 0.054 0.057 0.060 \n",
            "                                         model/h14/attn/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h14/ln_2/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h14/ln_2/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                            model/h14/mlp/c_fc/w - [  768,  3072,     1], type =    f16 size =     9.00 MB ->     1.55 MB | hist: 0.078 0.063 0.061 0.058 0.059 0.062 0.067 0.071 0.078 0.064 0.060 0.056 0.054 0.054 0.057 0.060 \n",
            "                                            model/h14/mlp/c_fc/b - [ 3072,     1,     1], type =    f32 size =    0.012 MB\n",
            "                                          model/h14/mlp/c_proj/w - [ 3072,   768,     1], type =    f16 size =     9.00 MB ->     1.55 MB | hist: 0.080 0.063 0.060 0.057 0.057 0.061 0.067 0.072 0.081 0.065 0.059 0.055 0.052 0.054 0.057 0.061 \n",
            "                                          model/h14/mlp/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h15/ln_1/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h15/ln_1/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                         model/h15/attn/c_attn/w - [  768,  2304,     1], type =    f16 size =     6.75 MB ->     1.16 MB | hist: 0.080 0.063 0.059 0.057 0.057 0.061 0.067 0.072 0.081 0.065 0.059 0.055 0.053 0.054 0.058 0.060 \n",
            "                                         model/h15/attn/c_attn/b - [ 2304,     1,     1], type =    f32 size =    0.009 MB\n",
            "                                         model/h15/attn/c_proj/w - [  768,   768,     1], type =    f16 size =     2.25 MB ->     0.39 MB | hist: 0.079 0.063 0.060 0.058 0.058 0.062 0.066 0.071 0.079 0.065 0.060 0.055 0.053 0.054 0.057 0.060 \n",
            "                                         model/h15/attn/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h15/ln_2/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h15/ln_2/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                            model/h15/mlp/c_fc/w - [  768,  3072,     1], type =    f16 size =     9.00 MB ->     1.55 MB | hist: 0.078 0.062 0.060 0.058 0.059 0.062 0.067 0.071 0.078 0.063 0.060 0.056 0.054 0.055 0.057 0.059 \n",
            "                                            model/h15/mlp/c_fc/b - [ 3072,     1,     1], type =    f32 size =    0.012 MB\n",
            "                                          model/h15/mlp/c_proj/w - [ 3072,   768,     1], type =    f16 size =     9.00 MB ->     1.55 MB | hist: 0.080 0.063 0.060 0.057 0.057 0.061 0.067 0.072 0.080 0.064 0.060 0.055 0.053 0.054 0.057 0.061 \n",
            "                                          model/h15/mlp/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h16/ln_1/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h16/ln_1/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                         model/h16/attn/c_attn/w - [  768,  2304,     1], type =    f16 size =     6.75 MB ->     1.16 MB | hist: 0.080 0.062 0.059 0.057 0.057 0.061 0.067 0.073 0.081 0.065 0.059 0.055 0.053 0.053 0.058 0.061 \n",
            "                                         model/h16/attn/c_attn/b - [ 2304,     1,     1], type =    f32 size =    0.009 MB\n",
            "                                         model/h16/attn/c_proj/w - [  768,   768,     1], type =    f16 size =     2.25 MB ->     0.39 MB | hist: 0.080 0.063 0.060 0.058 0.058 0.061 0.067 0.071 0.079 0.064 0.060 0.055 0.053 0.054 0.057 0.060 \n",
            "                                         model/h16/attn/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h16/ln_2/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h16/ln_2/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                            model/h16/mlp/c_fc/w - [  768,  3072,     1], type =    f16 size =     9.00 MB ->     1.55 MB | hist: 0.078 0.062 0.061 0.058 0.059 0.062 0.066 0.070 0.078 0.064 0.060 0.056 0.054 0.055 0.057 0.060 \n",
            "                                            model/h16/mlp/c_fc/b - [ 3072,     1,     1], type =    f32 size =    0.012 MB\n",
            "                                          model/h16/mlp/c_proj/w - [ 3072,   768,     1], type =    f16 size =     9.00 MB ->     1.55 MB | hist: 0.080 0.063 0.060 0.057 0.057 0.061 0.067 0.072 0.080 0.064 0.060 0.055 0.053 0.054 0.057 0.060 \n",
            "                                          model/h16/mlp/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h17/ln_1/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h17/ln_1/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                         model/h17/attn/c_attn/w - [  768,  2304,     1], type =    f16 size =     6.75 MB ->     1.16 MB | hist: 0.080 0.063 0.060 0.056 0.058 0.061 0.066 0.072 0.081 0.064 0.059 0.056 0.053 0.054 0.057 0.061 \n",
            "                                         model/h17/attn/c_attn/b - [ 2304,     1,     1], type =    f32 size =    0.009 MB\n",
            "                                         model/h17/attn/c_proj/w - [  768,   768,     1], type =    f16 size =     2.25 MB ->     0.39 MB | hist: 0.079 0.062 0.060 0.058 0.058 0.062 0.066 0.070 0.078 0.065 0.060 0.056 0.054 0.054 0.057 0.061 \n",
            "                                         model/h17/attn/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h17/ln_2/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h17/ln_2/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                            model/h17/mlp/c_fc/w - [  768,  3072,     1], type =    f16 size =     9.00 MB ->     1.55 MB | hist: 0.078 0.063 0.060 0.059 0.059 0.062 0.066 0.071 0.078 0.064 0.059 0.056 0.054 0.055 0.057 0.060 \n",
            "                                            model/h17/mlp/c_fc/b - [ 3072,     1,     1], type =    f32 size =    0.012 MB\n",
            "                                          model/h17/mlp/c_proj/w - [ 3072,   768,     1], type =    f16 size =     9.00 MB ->     1.55 MB | hist: 0.080 0.063 0.060 0.057 0.057 0.061 0.067 0.072 0.081 0.065 0.060 0.054 0.052 0.054 0.057 0.061 \n",
            "                                          model/h17/mlp/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h18/ln_1/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h18/ln_1/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                         model/h18/attn/c_attn/w - [  768,  2304,     1], type =    f16 size =     6.75 MB ->     1.16 MB | hist: 0.080 0.063 0.060 0.057 0.058 0.061 0.065 0.071 0.081 0.065 0.059 0.055 0.053 0.054 0.057 0.060 \n",
            "                                         model/h18/attn/c_attn/b - [ 2304,     1,     1], type =    f32 size =    0.009 MB\n",
            "                                         model/h18/attn/c_proj/w - [  768,   768,     1], type =    f16 size =     2.25 MB ->     0.39 MB | hist: 0.079 0.063 0.060 0.059 0.059 0.063 0.067 0.070 0.077 0.063 0.060 0.057 0.054 0.055 0.057 0.059 \n",
            "                                         model/h18/attn/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h18/ln_2/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h18/ln_2/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                            model/h18/mlp/c_fc/w - [  768,  3072,     1], type =    f16 size =     9.00 MB ->     1.55 MB | hist: 0.078 0.062 0.060 0.059 0.059 0.062 0.067 0.070 0.078 0.064 0.060 0.056 0.054 0.055 0.057 0.060 \n",
            "                                            model/h18/mlp/c_fc/b - [ 3072,     1,     1], type =    f32 size =    0.012 MB\n",
            "                                          model/h18/mlp/c_proj/w - [ 3072,   768,     1], type =    f16 size =     9.00 MB ->     1.55 MB | hist: 0.081 0.064 0.060 0.056 0.056 0.060 0.067 0.073 0.081 0.065 0.059 0.054 0.051 0.053 0.057 0.062 \n",
            "                                          model/h18/mlp/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h19/ln_1/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h19/ln_1/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                         model/h19/attn/c_attn/w - [  768,  2304,     1], type =    f16 size =     6.75 MB ->     1.16 MB | hist: 0.080 0.063 0.060 0.058 0.058 0.061 0.067 0.072 0.080 0.063 0.059 0.055 0.052 0.054 0.057 0.060 \n",
            "                                         model/h19/attn/c_attn/b - [ 2304,     1,     1], type =    f32 size =    0.009 MB\n",
            "                                         model/h19/attn/c_proj/w - [  768,   768,     1], type =    f16 size =     2.25 MB ->     0.39 MB | hist: 0.079 0.063 0.061 0.058 0.059 0.062 0.066 0.071 0.078 0.064 0.060 0.056 0.054 0.055 0.057 0.060 \n",
            "                                         model/h19/attn/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h19/ln_2/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h19/ln_2/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                            model/h19/mlp/c_fc/w - [  768,  3072,     1], type =    f16 size =     9.00 MB ->     1.55 MB | hist: 0.078 0.063 0.060 0.059 0.059 0.062 0.066 0.070 0.078 0.063 0.060 0.056 0.054 0.055 0.057 0.059 \n",
            "                                            model/h19/mlp/c_fc/b - [ 3072,     1,     1], type =    f32 size =    0.012 MB\n",
            "                                          model/h19/mlp/c_proj/w - [ 3072,   768,     1], type =    f16 size =     9.00 MB ->     1.55 MB | hist: 0.082 0.064 0.059 0.055 0.055 0.060 0.067 0.074 0.083 0.066 0.059 0.053 0.051 0.053 0.057 0.062 \n",
            "                                          model/h19/mlp/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                    model/ln_f/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                    model/ln_f/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                   model/lm_head - [  768, 49152,     1], type =    f16 size =   144.00 MB ->    24.75 MB | hist: 0.083 0.064 0.057 0.053 0.052 0.058 0.066 0.075 0.087 0.067 0.059 0.052 0.050 0.052 0.059 0.066 \n",
            "ggml_common_quantize_0: model size  =   852.77 MB\n",
            "ggml_common_quantize_0: quant size  =   167.08 MB | ftype = 8 (q5_0)\n",
            "ggml_common_quantize_0: hist: 0.081 0.064 0.059 0.055 0.056 0.060 0.066 0.073 0.083 0.066 0.059 0.054 0.051 0.053 0.058 0.063 \n",
            "\n",
            "main: quantize time =  1963.89 ms\n",
            "main:    total time =  1963.89 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd starcoder.cpp && ./quantize ../models/rahuldshetty/tiny-starcoder-instruct-ggml.bin ../quantized/tiny-starcoder-instruct-ggml-q8_0.bin 7"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Zpun-s0VI9M",
        "outputId": "69914a3b-4df5-4052-bce8-d8d80f4090d2"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "starcoder_model_quantize: loading model from '../models/rahuldshetty/tiny-starcoder-instruct-ggml.bin'\n",
            "starcoder_model_quantize: n_vocab     = 49152\n",
            "starcoder_model_quantize: n_ctx       = 8192\n",
            "starcoder_model_quantize: n_embd      = 768\n",
            "starcoder_model_quantize: n_head      = 12\n",
            "starcoder_model_quantize: n_layer     = 20\n",
            "starcoder_model_quantize: ftype (src) = 1\n",
            "starcoder_model_quantize: qntvr (src) = 0\n",
            "starcoder_model_quantize: ftype (dst) = 1007\n",
            "starcoder_model_quantize: qntvr (dst) = 1\n",
            "                                                       model/wte - [  768, 49152,     1], type =    f16 size =   144.00 MB ->    40.50 MB | hist: 0.000 0.026 0.017 0.027 0.042 0.063 0.088 0.114 0.254 0.113 0.087 0.061 0.041 0.026 0.016 0.025 \n",
            "                                                       model/wpe - [  768,  8192,     1], type =    f32 size =   24.000 MB\n",
            "                                                 model/h0/ln_1/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h0/ln_1/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                          model/h0/attn/c_attn/w - [  768,  2304,     1], type =    f16 size =     6.75 MB ->     1.90 MB | hist: 0.000 0.027 0.019 0.031 0.047 0.066 0.087 0.108 0.231 0.109 0.087 0.066 0.047 0.030 0.019 0.027 \n",
            "                                          model/h0/attn/c_attn/b - [ 2304,     1,     1], type =    f32 size =    0.009 MB\n",
            "                                          model/h0/attn/c_proj/w - [  768,   768,     1], type =    f16 size =     2.25 MB ->     0.63 MB | hist: 0.000 0.027 0.020 0.031 0.047 0.066 0.087 0.107 0.230 0.106 0.087 0.066 0.047 0.031 0.020 0.027 \n",
            "                                          model/h0/attn/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h0/ln_2/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h0/ln_2/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                             model/h0/mlp/c_fc/w - [  768,  3072,     1], type =    f16 size =     9.00 MB ->     2.53 MB | hist: 0.000 0.027 0.019 0.031 0.047 0.066 0.088 0.107 0.229 0.107 0.088 0.067 0.047 0.031 0.020 0.027 \n",
            "                                             model/h0/mlp/c_fc/b - [ 3072,     1,     1], type =    f32 size =    0.012 MB\n",
            "                                           model/h0/mlp/c_proj/w - [ 3072,   768,     1], type =    f16 size =     9.00 MB ->     2.53 MB | hist: 0.000 0.027 0.019 0.030 0.046 0.065 0.087 0.108 0.237 0.108 0.087 0.065 0.045 0.030 0.019 0.027 \n",
            "                                           model/h0/mlp/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h1/ln_1/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h1/ln_1/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                          model/h1/attn/c_attn/w - [  768,  2304,     1], type =    f16 size =     6.75 MB ->     1.90 MB | hist: 0.000 0.026 0.018 0.031 0.046 0.064 0.088 0.107 0.235 0.109 0.089 0.066 0.045 0.030 0.019 0.027 \n",
            "                                          model/h1/attn/c_attn/b - [ 2304,     1,     1], type =    f32 size =    0.009 MB\n",
            "                                          model/h1/attn/c_proj/w - [  768,   768,     1], type =    f16 size =     2.25 MB ->     0.63 MB | hist: 0.000 0.027 0.018 0.029 0.045 0.064 0.087 0.109 0.241 0.110 0.086 0.064 0.045 0.029 0.019 0.027 \n",
            "                                          model/h1/attn/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h1/ln_2/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h1/ln_2/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                             model/h1/mlp/c_fc/w - [  768,  3072,     1], type =    f16 size =     9.00 MB ->     2.53 MB | hist: 0.000 0.026 0.019 0.030 0.045 0.065 0.087 0.107 0.236 0.108 0.088 0.066 0.046 0.031 0.019 0.027 \n",
            "                                             model/h1/mlp/c_fc/b - [ 3072,     1,     1], type =    f32 size =    0.012 MB\n",
            "                                           model/h1/mlp/c_proj/w - [ 3072,   768,     1], type =    f16 size =     9.00 MB ->     2.53 MB | hist: 0.000 0.026 0.018 0.028 0.043 0.063 0.086 0.111 0.250 0.111 0.086 0.062 0.043 0.028 0.018 0.026 \n",
            "                                           model/h1/mlp/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h2/ln_1/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h2/ln_1/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                          model/h2/attn/c_attn/w - [  768,  2304,     1], type =    f16 size =     6.75 MB ->     1.90 MB | hist: 0.000 0.026 0.018 0.029 0.044 0.064 0.087 0.110 0.242 0.110 0.088 0.063 0.045 0.029 0.018 0.026 \n",
            "                                          model/h2/attn/c_attn/b - [ 2304,     1,     1], type =    f32 size =    0.009 MB\n",
            "                                          model/h2/attn/c_proj/w - [  768,   768,     1], type =    f16 size =     2.25 MB ->     0.63 MB | hist: 0.000 0.025 0.017 0.028 0.043 0.063 0.085 0.109 0.249 0.112 0.088 0.063 0.044 0.029 0.018 0.028 \n",
            "                                          model/h2/attn/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h2/ln_2/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h2/ln_2/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                             model/h2/mlp/c_fc/w - [  768,  3072,     1], type =    f16 size =     9.00 MB ->     2.53 MB | hist: 0.000 0.026 0.019 0.030 0.045 0.064 0.087 0.108 0.236 0.108 0.088 0.066 0.046 0.031 0.019 0.027 \n",
            "                                             model/h2/mlp/c_fc/b - [ 3072,     1,     1], type =    f32 size =    0.012 MB\n",
            "                                           model/h2/mlp/c_proj/w - [ 3072,   768,     1], type =    f16 size =     9.00 MB ->     2.53 MB | hist: 0.000 0.026 0.017 0.028 0.043 0.063 0.087 0.111 0.252 0.110 0.086 0.062 0.043 0.028 0.017 0.026 \n",
            "                                           model/h2/mlp/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h3/ln_1/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h3/ln_1/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                          model/h3/attn/c_attn/w - [  768,  2304,     1], type =    f16 size =     6.75 MB ->     1.90 MB | hist: 0.000 0.026 0.017 0.027 0.042 0.060 0.084 0.111 0.264 0.112 0.084 0.060 0.042 0.028 0.017 0.026 \n",
            "                                          model/h3/attn/c_attn/b - [ 2304,     1,     1], type =    f32 size =    0.009 MB\n",
            "                                          model/h3/attn/c_proj/w - [  768,   768,     1], type =    f16 size =     2.25 MB ->     0.63 MB | hist: 0.000 0.027 0.019 0.031 0.047 0.067 0.088 0.106 0.228 0.107 0.088 0.067 0.047 0.031 0.020 0.027 \n",
            "                                          model/h3/attn/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h3/ln_2/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h3/ln_2/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                             model/h3/mlp/c_fc/w - [  768,  3072,     1], type =    f16 size =     9.00 MB ->     2.53 MB | hist: 0.000 0.026 0.019 0.030 0.045 0.065 0.087 0.108 0.233 0.108 0.088 0.066 0.047 0.031 0.019 0.028 \n",
            "                                             model/h3/mlp/c_fc/b - [ 3072,     1,     1], type =    f32 size =    0.012 MB\n",
            "                                           model/h3/mlp/c_proj/w - [ 3072,   768,     1], type =    f16 size =     9.00 MB ->     2.53 MB | hist: 0.000 0.026 0.017 0.028 0.043 0.063 0.087 0.111 0.250 0.111 0.087 0.063 0.043 0.028 0.017 0.026 \n",
            "                                           model/h3/mlp/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h4/ln_1/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h4/ln_1/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                          model/h4/attn/c_attn/w - [  768,  2304,     1], type =    f16 size =     6.75 MB ->     1.90 MB | hist: 0.000 0.027 0.017 0.027 0.042 0.061 0.086 0.111 0.255 0.112 0.086 0.062 0.042 0.028 0.017 0.026 \n",
            "                                          model/h4/attn/c_attn/b - [ 2304,     1,     1], type =    f32 size =    0.009 MB\n",
            "                                          model/h4/attn/c_proj/w - [  768,   768,     1], type =    f16 size =     2.25 MB ->     0.63 MB | hist: 0.000 0.028 0.020 0.031 0.047 0.066 0.088 0.106 0.227 0.107 0.088 0.067 0.047 0.031 0.020 0.027 \n",
            "                                          model/h4/attn/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h4/ln_2/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h4/ln_2/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                             model/h4/mlp/c_fc/w - [  768,  3072,     1], type =    f16 size =     9.00 MB ->     2.53 MB | hist: 0.000 0.026 0.019 0.030 0.046 0.066 0.087 0.107 0.231 0.108 0.088 0.067 0.047 0.031 0.020 0.028 \n",
            "                                             model/h4/mlp/c_fc/b - [ 3072,     1,     1], type =    f32 size =    0.012 MB\n",
            "                                           model/h4/mlp/c_proj/w - [ 3072,   768,     1], type =    f16 size =     9.00 MB ->     2.53 MB | hist: 0.000 0.026 0.018 0.029 0.044 0.064 0.088 0.110 0.242 0.110 0.088 0.064 0.044 0.029 0.018 0.026 \n",
            "                                           model/h4/mlp/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h5/ln_1/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h5/ln_1/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                          model/h5/attn/c_attn/w - [  768,  2304,     1], type =    f16 size =     6.75 MB ->     1.90 MB | hist: 0.000 0.027 0.018 0.029 0.044 0.064 0.086 0.110 0.244 0.110 0.086 0.064 0.044 0.029 0.019 0.026 \n",
            "                                          model/h5/attn/c_attn/b - [ 2304,     1,     1], type =    f32 size =    0.009 MB\n",
            "                                          model/h5/attn/c_proj/w - [  768,   768,     1], type =    f16 size =     2.25 MB ->     0.63 MB | hist: 0.000 0.027 0.020 0.032 0.047 0.066 0.088 0.107 0.227 0.106 0.087 0.067 0.047 0.031 0.020 0.027 \n",
            "                                          model/h5/attn/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h5/ln_2/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h5/ln_2/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                             model/h5/mlp/c_fc/w - [  768,  3072,     1], type =    f16 size =     9.00 MB ->     2.53 MB | hist: 0.000 0.026 0.019 0.030 0.046 0.065 0.087 0.107 0.231 0.108 0.088 0.066 0.047 0.031 0.020 0.028 \n",
            "                                             model/h5/mlp/c_fc/b - [ 3072,     1,     1], type =    f32 size =    0.012 MB\n",
            "                                           model/h5/mlp/c_proj/w - [ 3072,   768,     1], type =    f16 size =     9.00 MB ->     2.53 MB | hist: 0.000 0.026 0.018 0.029 0.045 0.065 0.088 0.109 0.238 0.108 0.088 0.065 0.045 0.029 0.018 0.027 \n",
            "                                           model/h5/mlp/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h6/ln_1/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h6/ln_1/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                          model/h6/attn/c_attn/w - [  768,  2304,     1], type =    f16 size =     6.75 MB ->     1.90 MB | hist: 0.000 0.027 0.019 0.029 0.045 0.064 0.088 0.109 0.240 0.109 0.088 0.064 0.045 0.030 0.019 0.026 \n",
            "                                          model/h6/attn/c_attn/b - [ 2304,     1,     1], type =    f32 size =    0.009 MB\n",
            "                                          model/h6/attn/c_proj/w - [  768,   768,     1], type =    f16 size =     2.25 MB ->     0.63 MB | hist: 0.000 0.027 0.020 0.032 0.047 0.067 0.087 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "                                          model/h6/attn/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h6/ln_2/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h6/ln_2/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                             model/h6/mlp/c_fc/w - [  768,  3072,     1], type =    f16 size =     9.00 MB ->     2.53 MB | hist: 0.000 0.027 0.019 0.031 0.046 0.066 0.087 0.107 0.230 0.107 0.088 0.066 0.047 0.031 0.020 0.027 \n",
            "                                             model/h6/mlp/c_fc/b - [ 3072,     1,     1], type =    f32 size =    0.012 MB\n",
            "                                           model/h6/mlp/c_proj/w - [ 3072,   768,     1], type =    f16 size =     9.00 MB ->     2.53 MB | hist: 0.000 0.026 0.018 0.029 0.045 0.065 0.088 0.109 0.237 0.109 0.088 0.065 0.045 0.029 0.018 0.027 \n",
            "                                           model/h6/mlp/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h7/ln_1/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h7/ln_1/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                          model/h7/attn/c_attn/w - [  768,  2304,     1], type =    f16 size =     6.75 MB ->     1.90 MB | hist: 0.000 0.027 0.019 0.030 0.045 0.066 0.088 0.109 0.233 0.108 0.087 0.067 0.045 0.030 0.019 0.026 \n",
            "                                          model/h7/attn/c_attn/b - [ 2304,     1,     1], type =    f32 size =    0.009 MB\n",
            "                                          model/h7/attn/c_proj/w - [  768,   768,     1], type =    f16 size =     2.25 MB ->     0.63 MB | hist: 0.000 0.027 0.019 0.031 0.047 0.066 0.088 0.108 0.229 0.107 0.088 0.066 0.047 0.031 0.019 0.027 \n",
            "                                          model/h7/attn/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h7/ln_2/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h7/ln_2/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                             model/h7/mlp/c_fc/w - [  768,  3072,     1], type =    f16 size =     9.00 MB ->     2.53 MB | hist: 0.000 0.027 0.019 0.031 0.047 0.066 0.088 0.107 0.230 0.107 0.088 0.066 0.047 0.031 0.020 0.027 \n",
            "                                             model/h7/mlp/c_fc/b - [ 3072,     1,     1], type =    f32 size =    0.012 MB\n",
            "                                           model/h7/mlp/c_proj/w - [ 3072,   768,     1], type =    f16 size =     9.00 MB ->     2.53 MB | hist: 0.000 0.026 0.018 0.029 0.045 0.065 0.088 0.110 0.239 0.109 0.088 0.065 0.045 0.029 0.018 0.026 \n",
            "                                           model/h7/mlp/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h8/ln_1/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h8/ln_1/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                          model/h8/attn/c_attn/w - [  768,  2304,     1], type =    f16 size =     6.75 MB ->     1.90 MB | hist: 0.000 0.027 0.019 0.031 0.046 0.066 0.087 0.107 0.232 0.107 0.087 0.066 0.046 0.031 0.019 0.027 \n",
            "                                          model/h8/attn/c_attn/b - [ 2304,     1,     1], type =    f32 size =    0.009 MB\n",
            "                                          model/h8/attn/c_proj/w - [  768,   768,     1], type =    f16 size =     2.25 MB ->     0.63 MB | hist: 0.000 0.027 0.020 0.031 0.048 0.067 0.088 0.105 0.226 0.107 0.088 0.067 0.048 0.031 0.020 0.027 \n",
            "                                          model/h8/attn/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h8/ln_2/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h8/ln_2/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                             model/h8/mlp/c_fc/w - [  768,  3072,     1], type =    f16 size =     9.00 MB ->     2.53 MB | hist: 0.000 0.027 0.019 0.031 0.047 0.066 0.088 0.107 0.229 0.107 0.088 0.067 0.047 0.031 0.020 0.027 \n",
            "                                             model/h8/mlp/c_fc/b - [ 3072,     1,     1], type =    f32 size =    0.012 MB\n",
            "                                           model/h8/mlp/c_proj/w - [ 3072,   768,     1], type =    f16 size =     9.00 MB ->     2.53 MB | hist: 0.000 0.026 0.019 0.030 0.045 0.066 0.088 0.109 0.235 0.109 0.088 0.066 0.046 0.030 0.019 0.027 \n",
            "                                           model/h8/mlp/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h9/ln_1/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h9/ln_1/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                          model/h9/attn/c_attn/w - [  768,  2304,     1], type =    f16 size =     6.75 MB ->     1.90 MB | hist: 0.000 0.027 0.018 0.029 0.045 0.065 0.087 0.110 0.239 0.108 0.087 0.064 0.046 0.030 0.019 0.026 \n",
            "                                          model/h9/attn/c_attn/b - [ 2304,     1,     1], type =    f32 size =    0.009 MB\n",
            "                                          model/h9/attn/c_proj/w - [  768,   768,     1], type =    f16 size =     2.25 MB ->     0.63 MB | hist: 0.000 0.027 0.019 0.031 0.047 0.067 0.088 0.107 0.231 0.107 0.088 0.066 0.046 0.030 0.019 0.027 \n",
            "                                          model/h9/attn/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h9/ln_2/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                 model/h9/ln_2/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                             model/h9/mlp/c_fc/w - [  768,  3072,     1], type =    f16 size =     9.00 MB ->     2.53 MB | hist: 0.000 0.027 0.020 0.031 0.046 0.066 0.088 0.107 0.229 0.107 0.088 0.067 0.047 0.031 0.020 0.027 \n",
            "                                             model/h9/mlp/c_fc/b - [ 3072,     1,     1], type =    f32 size =    0.012 MB\n",
            "                                           model/h9/mlp/c_proj/w - [ 3072,   768,     1], type =    f16 size =     9.00 MB ->     2.53 MB | hist: 0.000 0.026 0.019 0.030 0.045 0.065 0.088 0.109 0.236 0.109 0.088 0.066 0.045 0.030 0.019 0.026 \n",
            "                                           model/h9/mlp/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h10/ln_1/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h10/ln_1/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                         model/h10/attn/c_attn/w - [  768,  2304,     1], type =    f16 size =     6.75 MB ->     1.90 MB | hist: 0.000 0.026 0.018 0.029 0.044 0.065 0.086 0.109 0.244 0.109 0.087 0.064 0.044 0.030 0.018 0.026 \n",
            "                                         model/h10/attn/c_attn/b - [ 2304,     1,     1], type =    f32 size =    0.009 MB\n",
            "                                         model/h10/attn/c_proj/w - [  768,   768,     1], type =    f16 size =     2.25 MB ->     0.63 MB | hist: 0.000 0.027 0.020 0.031 0.047 0.066 0.088 0.107 0.227 0.107 0.088 0.066 0.047 0.031 0.020 0.027 \n",
            "                                         model/h10/attn/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h10/ln_2/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h10/ln_2/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                            model/h10/mlp/c_fc/w - [  768,  3072,     1], type =    f16 size =     9.00 MB ->     2.53 MB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.107 0.228 0.107 0.088 0.067 0.047 0.031 0.020 0.028 \n",
            "                                            model/h10/mlp/c_fc/b - [ 3072,     1,     1], type =    f32 size =    0.012 MB\n",
            "                                          model/h10/mlp/c_proj/w - [ 3072,   768,     1], type =    f16 size =     9.00 MB ->     2.53 MB | hist: 0.000 0.027 0.019 0.030 0.046 0.066 0.088 0.108 0.233 0.108 0.088 0.066 0.046 0.030 0.019 0.027 \n",
            "                                          model/h10/mlp/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h11/ln_1/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h11/ln_1/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                         model/h11/attn/c_attn/w - [  768,  2304,     1], type =    f16 size =     6.75 MB ->     1.90 MB | hist: 0.000 0.026 0.018 0.030 0.046 0.064 0.087 0.108 0.240 0.109 0.086 0.065 0.045 0.029 0.019 0.026 \n",
            "                                         model/h11/attn/c_attn/b - [ 2304,     1,     1], type =    f32 size =    0.009 MB\n",
            "                                         model/h11/attn/c_proj/w - [  768,   768,     1], type =    f16 size =     2.25 MB ->     0.63 MB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.087 0.107 0.227 0.107 0.088 0.067 0.047 0.032 0.019 0.027 \n",
            "                                         model/h11/attn/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h11/ln_2/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h11/ln_2/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                            model/h11/mlp/c_fc/w - [  768,  3072,     1], type =    f16 size =     9.00 MB ->     2.53 MB | hist: 0.000 0.027 0.019 0.031 0.047 0.067 0.088 0.106 0.227 0.107 0.088 0.067 0.047 0.031 0.020 0.028 \n",
            "                                            model/h11/mlp/c_fc/b - [ 3072,     1,     1], type =    f32 size =    0.012 MB\n",
            "                                          model/h11/mlp/c_proj/w - [ 3072,   768,     1], type =    f16 size =     9.00 MB ->     2.53 MB | hist: 0.000 0.027 0.019 0.030 0.046 0.065 0.088 0.108 0.234 0.108 0.088 0.066 0.046 0.030 0.019 0.027 \n",
            "                                          model/h11/mlp/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h12/ln_1/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h12/ln_1/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                         model/h12/attn/c_attn/w - [  768,  2304,     1], type =    f16 size =     6.75 MB ->     1.90 MB | hist: 0.000 0.027 0.020 0.030 0.046 0.065 0.087 0.109 0.233 0.107 0.088 0.067 0.046 0.030 0.019 0.026 \n",
            "                                         model/h12/attn/c_attn/b - [ 2304,     1,     1], type =    f32 size =    0.009 MB\n",
            "                                         model/h12/attn/c_proj/w - [  768,   768,     1], type =    f16 size =     2.25 MB ->     0.63 MB | hist: 0.000 0.027 0.020 0.031 0.047 0.066 0.088 0.106 0.228 0.107 0.088 0.067 0.047 0.031 0.019 0.027 \n",
            "                                         model/h12/attn/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h12/ln_2/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h12/ln_2/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                            model/h12/mlp/c_fc/w - [  768,  3072,     1], type =    f16 size =     9.00 MB ->     2.53 MB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.107 0.226 0.106 0.088 0.067 0.047 0.031 0.020 0.028 \n",
            "                                            model/h12/mlp/c_fc/b - [ 3072,     1,     1], type =    f32 size =    0.012 MB\n",
            "                                          model/h12/mlp/c_proj/w - [ 3072,   768,     1], type =    f16 size =     9.00 MB ->     2.53 MB | hist: 0.000 0.026 0.019 0.030 0.046 0.065 0.088 0.109 0.235 0.108 0.088 0.066 0.046 0.030 0.018 0.027 \n",
            "                                          model/h12/mlp/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h13/ln_1/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h13/ln_1/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                         model/h13/attn/c_attn/w - [  768,  2304,     1], type =    f16 size =     6.75 MB ->     1.90 MB | hist: 0.000 0.027 0.019 0.030 0.046 0.065 0.087 0.108 0.235 0.107 0.088 0.066 0.046 0.030 0.019 0.027 \n",
            "                                         model/h13/attn/c_attn/b - [ 2304,     1,     1], type =    f32 size =    0.009 MB\n",
            "                                         model/h13/attn/c_proj/w - [  768,   768,     1], type =    f16 size =     2.25 MB ->     0.63 MB | hist: 0.000 0.027 0.019 0.031 0.047 0.067 0.088 0.107 0.228 0.107 0.087 0.067 0.047 0.031 0.020 0.027 \n",
            "                                         model/h13/attn/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h13/ln_2/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h13/ln_2/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                            model/h13/mlp/c_fc/w - [  768,  3072,     1], type =    f16 size =     9.00 MB ->     2.53 MB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.107 0.226 0.107 0.088 0.067 0.047 0.031 0.020 0.027 \n",
            "                                            model/h13/mlp/c_fc/b - [ 3072,     1,     1], type =    f32 size =    0.012 MB\n",
            "                                          model/h13/mlp/c_proj/w - [ 3072,   768,     1], type =    f16 size =     9.00 MB ->     2.53 MB | hist: 0.000 0.027 0.019 0.030 0.046 0.066 0.088 0.108 0.231 0.108 0.088 0.066 0.046 0.031 0.019 0.027 \n",
            "                                          model/h13/mlp/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h14/ln_1/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h14/ln_1/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                         model/h14/attn/c_attn/w - [  768,  2304,     1], type =    f16 size =     6.75 MB ->     1.90 MB | hist: 0.000 0.027 0.019 0.030 0.046 0.065 0.089 0.108 0.232 0.107 0.088 0.065 0.047 0.030 0.020 0.026 \n",
            "                                         model/h14/attn/c_attn/b - [ 2304,     1,     1], type =    f32 size =    0.009 MB\n",
            "                                         model/h14/attn/c_proj/w - [  768,   768,     1], type =    f16 size =     2.25 MB ->     0.63 MB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.087 0.106 0.227 0.106 0.088 0.067 0.047 0.031 0.020 0.027 \n",
            "                                         model/h14/attn/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h14/ln_2/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h14/ln_2/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                            model/h14/mlp/c_fc/w - [  768,  3072,     1], type =    f16 size =     9.00 MB ->     2.53 MB | hist: 0.000 0.027 0.020 0.032 0.047 0.067 0.088 0.107 0.227 0.106 0.088 0.067 0.047 0.031 0.020 0.027 \n",
            "                                            model/h14/mlp/c_fc/b - [ 3072,     1,     1], type =    f32 size =    0.012 MB\n",
            "                                          model/h14/mlp/c_proj/w - [ 3072,   768,     1], type =    f16 size =     9.00 MB ->     2.53 MB | hist: 0.000 0.027 0.019 0.030 0.046 0.066 0.088 0.109 0.234 0.108 0.088 0.065 0.046 0.030 0.019 0.027 \n",
            "                                          model/h14/mlp/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h15/ln_1/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h15/ln_1/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                         model/h15/attn/c_attn/w - [  768,  2304,     1], type =    f16 size =     6.75 MB ->     1.90 MB | hist: 0.000 0.027 0.019 0.030 0.046 0.066 0.089 0.107 0.233 0.109 0.087 0.066 0.046 0.031 0.019 0.027 \n",
            "                                         model/h15/attn/c_attn/b - [ 2304,     1,     1], type =    f32 size =    0.009 MB\n",
            "                                         model/h15/attn/c_proj/w - [  768,   768,     1], type =    f16 size =     2.25 MB ->     0.63 MB | hist: 0.000 0.027 0.019 0.031 0.046 0.067 0.087 0.108 0.230 0.107 0.087 0.066 0.047 0.031 0.019 0.027 \n",
            "                                         model/h15/attn/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h15/ln_2/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h15/ln_2/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                            model/h15/mlp/c_fc/w - [  768,  3072,     1], type =    f16 size =     9.00 MB ->     2.53 MB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.107 0.226 0.106 0.088 0.067 0.047 0.031 0.020 0.027 \n",
            "                                            model/h15/mlp/c_fc/b - [ 3072,     1,     1], type =    f32 size =    0.012 MB\n",
            "                                          model/h15/mlp/c_proj/w - [ 3072,   768,     1], type =    f16 size =     9.00 MB ->     2.53 MB | hist: 0.000 0.027 0.019 0.030 0.046 0.065 0.088 0.108 0.234 0.108 0.088 0.066 0.046 0.030 0.019 0.026 \n",
            "                                          model/h15/mlp/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h16/ln_1/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h16/ln_1/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                         model/h16/attn/c_attn/w - [  768,  2304,     1], type =    f16 size =     6.75 MB ->     1.90 MB | hist: 0.000 0.027 0.019 0.030 0.046 0.065 0.087 0.108 0.235 0.107 0.087 0.065 0.046 0.030 0.019 0.027 \n",
            "                                         model/h16/attn/c_attn/b - [ 2304,     1,     1], type =    f32 size =    0.009 MB\n",
            "                                         model/h16/attn/c_proj/w - [  768,   768,     1], type =    f16 size =     2.25 MB ->     0.63 MB | hist: 0.000 0.027 0.019 0.031 0.047 0.066 0.088 0.108 0.230 0.107 0.088 0.066 0.047 0.031 0.020 0.027 \n",
            "                                         model/h16/attn/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h16/ln_2/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h16/ln_2/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                            model/h16/mlp/c_fc/w - [  768,  3072,     1], type =    f16 size =     9.00 MB ->     2.53 MB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "                                            model/h16/mlp/c_fc/b - [ 3072,     1,     1], type =    f32 size =    0.012 MB\n",
            "                                          model/h16/mlp/c_proj/w - [ 3072,   768,     1], type =    f16 size =     9.00 MB ->     2.53 MB | hist: 0.000 0.027 0.019 0.030 0.046 0.066 0.088 0.108 0.233 0.108 0.088 0.066 0.046 0.030 0.019 0.027 \n",
            "                                          model/h16/mlp/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h17/ln_1/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h17/ln_1/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                         model/h17/attn/c_attn/w - [  768,  2304,     1], type =    f16 size =     6.75 MB ->     1.90 MB | hist: 0.000 0.027 0.019 0.030 0.047 0.066 0.087 0.107 0.235 0.108 0.087 0.066 0.046 0.030 0.018 0.027 \n",
            "                                         model/h17/attn/c_attn/b - [ 2304,     1,     1], type =    f32 size =    0.009 MB\n",
            "                                         model/h17/attn/c_proj/w - [  768,   768,     1], type =    f16 size =     2.25 MB ->     0.63 MB | hist: 0.000 0.027 0.019 0.031 0.047 0.066 0.088 0.107 0.228 0.108 0.088 0.066 0.047 0.031 0.019 0.027 \n",
            "                                         model/h17/attn/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h17/ln_2/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h17/ln_2/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                            model/h17/mlp/c_fc/w - [  768,  3072,     1], type =    f16 size =     9.00 MB ->     2.53 MB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.047 0.031 0.020 0.028 \n",
            "                                            model/h17/mlp/c_fc/b - [ 3072,     1,     1], type =    f32 size =    0.012 MB\n",
            "                                          model/h17/mlp/c_proj/w - [ 3072,   768,     1], type =    f16 size =     9.00 MB ->     2.53 MB | hist: 0.000 0.026 0.018 0.030 0.045 0.065 0.088 0.109 0.236 0.109 0.088 0.066 0.045 0.029 0.018 0.027 \n",
            "                                          model/h17/mlp/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h18/ln_1/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h18/ln_1/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                         model/h18/attn/c_attn/w - [  768,  2304,     1], type =    f16 size =     6.75 MB ->     1.90 MB | hist: 0.000 0.027 0.019 0.031 0.046 0.066 0.087 0.106 0.235 0.107 0.086 0.066 0.046 0.031 0.019 0.027 \n",
            "                                         model/h18/attn/c_attn/b - [ 2304,     1,     1], type =    f32 size =    0.009 MB\n",
            "                                         model/h18/attn/c_proj/w - [  768,   768,     1], type =    f16 size =     2.25 MB ->     0.63 MB | hist: 0.000 0.027 0.020 0.031 0.047 0.068 0.088 0.106 0.225 0.106 0.088 0.067 0.047 0.032 0.020 0.027 \n",
            "                                         model/h18/attn/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h18/ln_2/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h18/ln_2/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                            model/h18/mlp/c_fc/w - [  768,  3072,     1], type =    f16 size =     9.00 MB ->     2.53 MB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.047 0.032 0.020 0.027 \n",
            "                                            model/h18/mlp/c_fc/b - [ 3072,     1,     1], type =    f32 size =    0.012 MB\n",
            "                                          model/h18/mlp/c_proj/w - [ 3072,   768,     1], type =    f16 size =     9.00 MB ->     2.53 MB | hist: 0.000 0.026 0.018 0.029 0.044 0.065 0.088 0.110 0.239 0.110 0.088 0.065 0.044 0.029 0.018 0.026 \n",
            "                                          model/h18/mlp/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h19/ln_1/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h19/ln_1/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                         model/h19/attn/c_attn/w - [  768,  2304,     1], type =    f16 size =     6.75 MB ->     1.90 MB | hist: 0.000 0.027 0.020 0.031 0.047 0.065 0.088 0.107 0.231 0.107 0.087 0.066 0.047 0.032 0.020 0.027 \n",
            "                                         model/h19/attn/c_attn/b - [ 2304,     1,     1], type =    f32 size =    0.009 MB\n",
            "                                         model/h19/attn/c_proj/w - [  768,   768,     1], type =    f16 size =     2.25 MB ->     0.63 MB | hist: 0.000 0.027 0.020 0.031 0.047 0.066 0.088 0.106 0.227 0.106 0.088 0.067 0.047 0.031 0.020 0.027 \n",
            "                                         model/h19/attn/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h19/ln_2/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                model/h19/ln_2/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                            model/h19/mlp/c_fc/w - [  768,  3072,     1], type =    f16 size =     9.00 MB ->     2.53 MB | hist: 0.000 0.027 0.020 0.031 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "                                            model/h19/mlp/c_fc/b - [ 3072,     1,     1], type =    f32 size =    0.012 MB\n",
            "                                          model/h19/mlp/c_proj/w - [ 3072,   768,     1], type =    f16 size =     9.00 MB ->     2.53 MB | hist: 0.000 0.026 0.018 0.028 0.044 0.064 0.088 0.110 0.242 0.110 0.089 0.064 0.044 0.028 0.018 0.026 \n",
            "                                          model/h19/mlp/c_proj/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                    model/ln_f/g - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                    model/ln_f/b - [  768,     1,     1], type =    f32 size =    0.003 MB\n",
            "                                                   model/lm_head - [  768, 49152,     1], type =    f16 size =   144.00 MB ->    40.50 MB | hist: 0.000 0.026 0.017 0.027 0.042 0.063 0.088 0.114 0.254 0.113 0.087 0.061 0.041 0.026 0.016 0.025 \n",
            "ggml_common_quantize_0: model size  =   852.77 MB\n",
            "ggml_common_quantize_0: quant size  =   257.64 MB | ftype = 7 (q8_0)\n",
            "ggml_common_quantize_0: hist: 0.000 0.026 0.018 0.029 0.044 0.064 0.088 0.110 0.241 0.110 0.087 0.064 0.044 0.029 0.018 0.026 \n",
            "\n",
            "main: quantize time =  2832.81 ms\n",
            "main:    total time =  2832.82 ms\n"
          ]
        }
      ]
    }
  ]
}